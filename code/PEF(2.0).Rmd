---
title: "PEF(2.0)"
author: "Aidan Cox"
date: "11/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## LIBRARY LIST
```{r Library, echo=FALSE}
library(tidyverse)
library(curl)
library(readxl)
library(writexl)
library(PerformanceAnalytics)
library(tools)
library(ncdf4) # package for netcdf manipulation
library(raster) # package for raster manipulation
library(rgdal) # package for geospatial analysis
library(dsmextra) # package for calculating model extrapolation
library(magrittr)
library(pROC) # package for calculating AUC of logistic regression models
library(multcomp) # package for ANOVA post-hoc testing

# models #
library(mgcv)
library(lme4)

# clustering
library(cluster)
library(factoextra)
library(NbClust)

# mapping
library(rnaturalearth)
library(rnaturalearthdata)
library(rgeos)
library(ggspatial)
library(sf)
library(viridis)

world <- ne_countries(scale = "medium", returnclass = "sf")
islands <- st_read("~/Dropbox/PelagicEcosystemFunction_21/land_area.shp")
transect <- st_read("~/Dropbox/PelagicEcosystemFunction_21/transect_line.shp")
```

## EXTRACTING TIDAL DATA

Here is a list of the tide height and current stations within my study area and their locations:

```{r Stations}
## Current Prediction Stations for Consideration
current_stations <- data.frame(
  station = c("PCT1966", "PCT1971", "PUG1728", "PUG1727", "PUG1702", "PUG1730", "PCT2006", "PUG1729", "PCT2026", "PUG1731", "PUG1733", "PCT2046", "PUG1732", "PUG1704", "PUG1705", "PUG1706", "PCT2071", "PCT2076", "PCT2126", "PUG1707", "PUG1708", "PUG1712", "PCT1416", "PUG1742", "PUG1703", "PCT2191", "PUG1746", "PUG1745", "PUG1723", "PUG1721", "PUG1720", "PUG1719", "PUG1715", "PUG1722", "PUG1744", "PUG1724", "PUG1718", "PCT2266", "PUG1716", "PCT2281"),
  location = c("Iceberg Pass", "Colville Island", "Point Colville", "Lawson Reef", "Rosario Strait", "Lopez Pass", "Burrows Bay", "Belle Rock", "Green Point", "Fontleroy Light", "Thatcher Pass", "Frost Willow Island", "Strawberry Island", "Peavine Pass", "Obstruction Pass", "Peapod Rocks", "Barnes Island", "Raccoon Island", "Towhead Island", "Sinclair Island", "Lawrence Point", "Parker Reef", "Cattle Point", "Cattle Point 2", "SJC South", "King's Point", "Pear Point", "Point George", "Upright Channel", "Wasp Passage", "Spring Passage", "Spieden Channel", "President Channel", "Harney Passage", "Discovery Island", "Lime Kiln", "Kellett Bluff", "John's Island", "Waldron Island", "Point Hammond" ),
  latitude = c(48.3833, 48.4000, 48.4181, 48.4125, 48.4581, 48.4797, 48.4628, 48.4968, 48.5047, 48.5216, 48.5274, 48.5392, 48.5610, 48.5871, 48.6033, 48.6224, 48.6858, 48.6122, 48.6442, 48.6794, 48.7326, 48.4000, 48.3840, 48.4344, 48.4610, 48.4833, 48.5114, 48.5567, 48.5538, 48.5925, 48.6115, 48.6278, 48.6734, 48.5897, 48.4521, 48.4980, 48.5887, 48.6833, 48.7042, 48.7320 ),
  longitude = c(-122.9167, -122.8167, -122.7812, -122.7403, -122.7501, - 122.8189, -122.6828, -122.7308, -122.7062, -122.7707, -122.8040, -122.8308, -122.7543, -122.8193, -122.8127, -122.7476, -122.7888, -122.7022, -122.6587, -122.7147, -122.8864, -123.0000, -123.0157, -122.9466, -122.9520, -122.9558, -122.9529, -122.9985, -122.9226, -122.9896, -123.0341, -123.1116, -123.0060, -122.9217, -123.1554, -123.1599, -123.2258, -123.1500, -123.1048, -123.0253)
)

## Map Station Locations
ggplot() +
  geom_sf(data = world, color = "black") +
  coord_sf(xlim = c(-124, -122), ylim = c(48,49), expand = FALSE) +
  
  geom_point(data = current_stations, aes(longitude, latitude, color = location)) +
  
  xlab("Longitude") +
  ylab("Latitude") +
  theme_minimal() +
  theme(legend.position = "none")
```

# Tidal Data Extraction
FOR INFORMATION ON NOAA CO-OPS API, READ: https://api.tidesandcurrents.noaa.gov/api/prod/

Extract tide current predictions (knots) from 40 tidal current stations within the study area:

```{r Tide Current Extraction}
#The parts of the url to be assembled:
url1 = "https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?begin_date="
url2 = "&end_date="
url3 = "&station=" #return stationId
url4 = "&product=currents" #return product
url5 = "&datum=mllw" #return datum
url6 = "&units=metric" #return units
url7 = "&time_zone=lst" #return time zone
url8 = "&application=univer_washington" #return application
url9 = "&format=csv" #return format
### 

stations = c("PCT1966", "PCT1971", "PUG1728", "PUG1727", "PUG1702", "PUG1730", "PCT2006", "PUG1729", "PCT2026", "PUG1731", "PUG1733", "PCT2046", "PUG1732", "PUG1704", "PUG1705", "PUG1706", "PCT2071", "PCT2076", "PCT2126", "PUG1707", "PUG1708", "PUG1712", "PCT1416", "PUG1742", "PUG1703", "PCT2191", "PUG1746", "PUG1745", "PUG1723", "PUG1721", "PUG1720", "PUG1719", "PUG1715", "PUG1722", "PUG1744", "PUG1724", "PUG1718", "PCT2266", "PUG1716", "PCT2281")

## 2019
dir <- "~/Desktop/PEF/Tides/2019" # set file-path

# 2019 October - November
for(i in 1:length(stations)) {
  begin_date <- 20191020
  end_date <- 20191120
  assign(station, i)
  urltotal <- paste(url1,begin_date,url2,end_date,url3,station,url4,url6,url7,url8,url9,sep ="")
  filename <- paste("2019", stations[i], "oct-nov", sep = "_")
  tmp <- tempfile(pattern = filename, tmpdir = dir, fileext = ".csv")
  curl_download(urltotal, tmp)
}

# 2019 November - December
for(i in 1:length(stations)) {
  begin_date <- 20191121
  end_date <- 20191210
  assign(station, i)
  urltotal <- paste(url1,begin_date,url2,end_date,url3,station,url4,url6,url7,url8,url9,sep ="")
  filename <- paste("2019", stations[i], "nov-dec", sep = "_")
  tmp <- tempfile(pattern = filename, tmpdir = dir, fileext = ".csv")
  curl_download(urltotal, tmp)
}

## 2020
dir <- "~/Desktop/PEF/Tides/2020"

# 2020 October-November
for(i in 1:length(stations)) {
  begin_date <- 20201001
  end_date <- 20201101
  assign(station, i)
  urltotal <- paste(url1,begin_date,url2,end_date,url3,station,url4,url6,url7,url8,url9,sep ="")
  filename <- paste("2020", stations[i], "oct-nov", sep = "_")
  tmp <- tempfile(pattern = filename, tmpdir = dir, fileext = ".csv")
  curl_download(urltotal, tmp)
}

# 2020 November-December
for(i in 1:length(stations)) {
  begin_date <- 20201101
  end_date <- 20201201
  assign(station, i)
  urltotal <- paste(url1,begin_date,url2,end_date,url3,station,url4,url6,url7,url8,url9,sep ="")
  filename <- paste("2020", stations[i], "nov-dec", sep = "_")
  tmp <- tempfile(pattern = filename, tmpdir = dir, fileext = ".csv")
  curl_download(urltotal, tmp)
}

## 2021
dir <- "~/Desktop/PEF/Tides/2021"

# 2021 October-November
for(i in 1:length(stations)) {
  begin_date <- 20211001
  end_date <- 20211101
  assign(station, i)
  urltotal <- paste(url1,begin_date,url2,end_date,url3,station,url4,url6,url7,url8,url9,sep ="")
  filename <- paste("2021", stations[i], "oct-nov", sep = "_")
  tmp <- tempfile(pattern = filename, tmpdir = dir, fileext = ".csv")
  curl_download(urltotal, tmp)
}

# 2021 November-December
for(i in 1:length(stations)) {
  begin_date <- 20211101
  end_date <- 20211201
  assign(station, i)
  urltotal <- paste(url1,begin_date,url2,end_date,url3,station,url4,url6,url7,url8,url9,sep ="")
  filename <- paste("2021", stations[i], "nov-dec", sep = "_")
  tmp <- tempfile(pattern = filename, tmpdir = dir, fileext = ".csv")
  curl_download(urltotal, tmp)
}
```

# Format Tidal Current CSVs
Take raw tidal current data from NOAA and calculate the daily amplitude of tidal current:
```{r Format Current CSV, echo=FALSE}
## 2019 ##
fList <- list.files("~/Desktop/PEF/Tides/2019/", full.names = T) # update with file path from chunk 4

current_amplitudes_2019 <- data.frame() 

for(i in 1:length(fList)) {
  currents <- read_csv(fList[i]) 
  currents <- currents %>% mutate(
    DateTime = `Date_Time (LST/LDT)`
  )
  currents <- currents %>% separate(DateTime, into = c("Date", "Time"), sep = "([ ])")
  currents <- currents %>% separate(Date, into = c("Year", "Month", "Day"), sep = "([-])")
  currents <- currents %>% filter(Month == 10 | Month == 11) # select only data from October and November
  currents <- currents %>% filter(`Speed (knots)` != "-")
  currents$`Speed (knots)` <- as.numeric(currents$`Speed (knots)`)
  currents <- currents %>% group_by(Month, Day, Event) %>%
    top_n(1, abs(`Speed (knots)`))
  currents <- currents %>% dplyr::select(
    Year, Month, Day, Event, `Speed (knots)`
  )
  currents <- distinct(currents)
  currents$`Speed (knots)` <- abs(currents$`Speed (knots)`)
  currents <- currents %>% group_by(Month, Day) %>%
    mutate(Amplitude = `Speed (knots)` + lag(`Speed (knots)`))
  currents$Station <- fList[i]
  currents <- currents %>% separate(Station, into = c("Trash1", "Trash2", "Trash3", "Trash4", "Trash5", "Trash6", "Trash7", "Trash8", "Station"), sep = "([/])")
  currents <- currents %>% separate(Station, into = c("Station", "Trash9", "Trash10", "Trash11"), sep = "([_])")
  currents <- currents %>% dplyr::select(Year, Month, Day, Amplitude, Station)
  currents <- currents %>% filter(Amplitude != 0)
  name <- paste(currents[1,1], currents[1,5], sep = "_")
  #name <- paste(currents[1,1], currents[1,5], sep = "_")
  #write_csv(currents, paste("~/Desktop/PEF/2019/", name, ".csv", sep = ""))
  currents <- ungroup(currents)
  current_amplitudes_2019 <- rbind(currents, current_amplitudes_2019) # update with year x2
  rm(currents, name)
  print(i)
}
current_amplitudes_2019$Year <- "2019" 

## 2020 ##
fList <- list.files("~/Desktop/PEF/Tides/2020/", full.names = T) # update with file path from chunk 4

current_amplitudes_2020 <- data.frame() 

for(i in 1:length(fList)) {
  currents <- read_csv(fList[i]) 
  currents <- currents %>% mutate(
    DateTime = `Date_Time (LST/LDT)`
  )
  currents <- currents %>% separate(DateTime, into = c("Date", "Time"), sep = "([ ])")
  currents <- currents %>% separate(Date, into = c("Year", "Month", "Day"), sep = "([-])")
  currents <- currents %>% filter(Month == 10 | Month == 11) # select only data from October and November
  currents <- currents %>% filter(`Speed (knots)` != "-")
  currents$`Speed (knots)` <- as.numeric(currents$`Speed (knots)`)
  currents <- currents %>% group_by(Month, Day, Event) %>%
    top_n(1, abs(`Speed (knots)`))
  currents <- currents %>% dplyr::select(
    Year, Month, Day, Event, `Speed (knots)`
  )
  currents <- distinct(currents)
  currents$`Speed (knots)` <- abs(currents$`Speed (knots)`)
  currents <- currents %>% group_by(Month, Day) %>%
    mutate(Amplitude = `Speed (knots)` + lag(`Speed (knots)`))
  currents$Station <- fList[i]
  currents <- currents %>% separate(Station, into = c("Trash1", "Trash2", "Trash3", "Trash4", "Trash5", "Trash6", "Trash7", "Trash8", "Station"), sep = "([/])")
  currents <- currents %>% separate(Station, into = c("Station", "Trash9", "Trash10", "Trash11"), sep = "([_])")
  currents <- currents %>% dplyr::select(Year, Month, Day, Amplitude, Station)
  currents <- currents %>% filter(Amplitude != 0)
  name <- paste(currents[1,1], currents[1,5], sep = "_")
  #name <- paste(currents[1,1], currents[1,5], sep = "_")
  #write_csv(currents, paste("~/Desktop/PEF/2020/", name, ".csv", sep = ""))
  currents <- ungroup(currents)
  current_amplitudes_2020 <- rbind(currents, current_amplitudes_2020) # update with year x2
  rm(currents, name)
  print(i)
}
current_amplitudes_2020$Year <- "2020" 

## 2021 ##
fList <- list.files("~/Desktop/PEF/Tides/2021/", full.names = T) # update with file path from chunk 4

current_amplitudes_2021 <- data.frame() 

for(i in 1:length(fList)) {
  currents <- read_csv(fList[i]) 
  currents <- currents %>% mutate(
    DateTime = `Date_Time (LST/LDT)`
  )
  currents <- currents %>% separate(DateTime, into = c("Date", "Time"), sep = "([ ])")
  currents <- currents %>% separate(Date, into = c("Year", "Month", "Day"), sep = "([-])")
  currents <- currents %>% filter(Month == 10 | Month == 11) # select only data from October and November
  currents <- currents %>% filter(`Speed (knots)` != "-")
  currents$`Speed (knots)` <- as.numeric(currents$`Speed (knots)`)
  currents <- currents %>% group_by(Month, Day, Event) %>%
    top_n(1, abs(`Speed (knots)`))
  currents <- currents %>% dplyr::select(
    Year, Month, Day, Event, `Speed (knots)`
  )
  currents <- distinct(currents)
  currents$`Speed (knots)` <- abs(currents$`Speed (knots)`)
  currents <- currents %>% group_by(Month, Day) %>%
    mutate(Amplitude = `Speed (knots)` + lag(`Speed (knots)`))
  currents$Station <- fList[i]
  currents <- currents %>% separate(Station, into = c("Trash1", "Trash2", "Trash3", "Trash4", "Trash5", "Trash6", "Trash7", "Trash8", "Station"), sep = "([/])")
  currents <- currents %>% separate(Station, into = c("Station", "Trash9", "Trash10", "Trash11"), sep = "([_])")
  currents <- currents %>% dplyr::select(Year, Month, Day, Amplitude, Station)
  currents <- currents %>% filter(Amplitude != 0)
  name <- paste(currents[1,1], currents[1,5], sep = "_")
  #name <- paste(currents[1,1], currents[1,5], sep = "_")
  #write_csv(currents, paste("~/Desktop/PEF/2021/", name, ".csv", sep = ""))
  currents <- ungroup(currents)
  current_amplitudes_2021 <- rbind(currents, current_amplitudes_2021) # update with year x2
  rm(currents, name)
  print(i)
}

current_amplitudes_2021$Year <- "2021" 
```

# Explore Correlations Between Data from Current Stations across years
Read this for a helpful reminder on ANOVA : http://www.sthda.com/english/wiki/one-way-anova-test-in-r

This code compares current data from matching stations from 2019, 2020, and 2021 to make sure that annual variability in current speeds is not significant:

```{r Tide Proxy Exploration}
proxy <- rbind(current_amplitudes_2019, current_amplitudes_2020, current_amplitudes_2021)

proxy$Year <- factor(proxy$Year)

# visually compare the data
ggplot(data = proxy) +
  geom_point(aes(x = Station, y = Amplitude, color = Year)) +
  coord_flip() +
  theme_classic() 

ggplot(data = proxy) +
  geom_jitter(aes(x = Year, y = Amplitude, color = Year), alpha = 0.5) +
  geom_boxplot(aes(x = Year, y = Amplitude), color = "black", fill = NA) +
  coord_flip() +
  theme_classic() 

proxy %>% group_by(Year) %>%
  summarise(
    count = n(),
    mean = mean(Amplitude, na.rm = TRUE),
    sd = sd(Amplitude, na.rm = TRUE)
  )

# ANOVA between years
tide.aov <- aov(Amplitude ~ Year, data = proxy)
summary(tide.aov)

rm(proxy, tide.aov) #cleanup

```
Based on these results, we can conclude that there is no significant difference between the seasonal average of tidal current amplitude across our study area

# Compile data from 2019-2020 by station
Calculate the average between 2019-2020 of the seasonal average value of tidal current amplitude at each current prediction station:

```{r 2019-2021 Seasonal Current Proxy}
tcur <- rbind(current_amplitudes_2019, current_amplitudes_2020, current_amplitudes_2021)
tcur <- tcur %>% group_by(Station) %>% summarise(tcur = mean(Amplitude))
```

## TIDE HEIGHT CHANGE EXTRACTION
```{r Delta Tide Height}
# 2017
delta_2017_11 <- read_csv("~/Desktop/PEF/Tide_Height/2017/2017_9449880_novacdc28a1638b.csv")
delta_2017_10 <- read_csv("~/Desktop/PEF/Tide_Height/2017/2017_9449880_octacdc6c288d25.csv")
delta_2017 <- rbind(delta_2017_10, delta_2017_11)
rm(delta_2017_10, delta_2017_11)
delta_2017$dup <- delta_2017$'Date Time'
delta_2017 <- delta_2017 %>% separate(dup, into = c("Date", "Time"), sep = "([ ])")
delta_2017$Date <- lubridate::ymd(delta_2017$Date)

# 2018
delta_2018_10 <- read_csv("~/Desktop/PEF/Tide_Height/2018/2018_9449880_octacdc74ea0e9.csv")
delta_2018_11 <- read_csv("~/Desktop/PEF/Tide_Height/2018/2018_9449880_novacdc5b6a46ec.csv")
delta_2018 <- rbind(delta_2018_10, delta_2018_11)
rm(delta_2018_10, delta_2018_11)
delta_2018$dup <- delta_2018$'Date Time'
delta_2018 <- delta_2018 %>% separate(dup, into = c("Date", "Time"), sep = "([ ])")
delta_2018$Date <- lubridate::ymd(delta_2018$Date)

# 2019
delta_2019_10 <- read_csv("~/Desktop/PEF/Tide_Height/2019/2019_9449880_octacdca1cd424.csv")
delta_2019_11 <- read_csv("~/Desktop/PEF/Tide_Height/2019/2019_9449880_novacdc5698ead8.csv")
delta_2019 <- rbind(delta_2019_10, delta_2019_11)
rm(delta_2019_10, delta_2019_11)
delta_2019$dup <- delta_2019$'Date Time'
delta_2019 <- delta_2019 %>% separate(dup, into = c("Date", "Time"), sep = "([ ])")
delta_2019$Date <- lubridate::ymd(delta_2019$Date)

# 2020
delta_2020_10 <- read_csv("~/Desktop/PEF/Tide_Height/2020/2020_9449880_octacdcebe78ce.csv")
delta_2020_11 <- read_csv("~/Desktop/PEF/Tide_Height/2020/2020_9449880_novacdc1a32ace5.csv")
delta_2020 <- rbind(delta_2020_10, delta_2020_11)
rm(delta_2020_10, delta_2020_11)
delta_2020$dup <- delta_2020$'Date Time'
delta_2020 <- delta_2020 %>% separate(dup, into = c("Date", "Time"), sep = "([ ])")
delta_2020$Date <- lubridate::ymd(delta_2020$Date)

# 2021
delta_2021_10 <- read_csv("~/Desktop/PEF/Tide_Height/2021/2021_9449880_octc51f32ede5b0.csv")
delta_2021_11 <- read_csv("~/Desktop/PEF/Tide_Height/2021/2021_9449880_novc51f77960932.csv")
delta_2021 <- rbind(delta_2021_10, delta_2021_11)
rm(delta_2021_10, delta_2021_11)
delta_2021$dup <- delta_2021$'Date Time'
delta_2021 <- delta_2021 %>% separate(dup, into = c("Date", "Time"), sep = "([ ])")
delta_2021$Date <- lubridate::ymd(delta_2021$Date)

delta.tide.height <- rbind(delta_2017, delta_2018, delta_2019, delta_2020, delta_2021)
rm(delta_2017, delta_2018, delta_2019, delta_2020, delta_2021)

# find lowest low and highest high for each day
d <- delta.tide.height %>% group_by(Date) %>% summarize(HH = max(Prediction), LL = min(Prediction))
delta.tide.height <- d %>% mutate(dth = (HH - LL))

```

## EXTRACTING LIVEOCEAN DATA

Open the netcdf files containing model data and read in the layers containing variables of interest:
```{r Open NetCDF File and Format Arrays}
nc_data <- nc_open("~/Dropbox/PelagicEcosystemFunction_21/cox_surf_2017.01.01_2021.10.30.nc")

lon <- ncvar_get(nc_data, "lon_rho")
lat <- ncvar_get(nc_data, "lat_rho")
t <- ncvar_get(nc_data, "ocean_time")

phyto.array <- ncvar_get(nc_data, "phytoplankton")
temp.array <- ncvar_get(nc_data, "temp")
sal.array <- ncvar_get(nc_data, "salt")

dim(phyto.array) # check the dimensions of the array, expected: 72(x) x 75(y) x 1764 days

# find out what value is input for misssing data
fillvalue <- ncatt_get(nc_data, "phytoplankton", "_FillValue")
phyto.array[phyto.array == fillvalue$value] <- NA

fillvalue <- ncatt_get(nc_data, "temp", "_FillValue")
temp.array[temp.array == fillvalue$value] <- NA

fillvalue <- ncatt_get(nc_data, "salt", "_FillValue")
sal.array[sal.array == fillvalue$value] <- NA

# close the NETCDF file
nc_close(nc_data) 
```

Determine spatial points used by the model and where they fall in the study grid:
```{r Apply LiveOcean points to study grid}
# get model points for extraction from LiveOcean #
extraction_points <- data.frame (
  longitude = c(lon),
  latitude = c(lat)
)

# read in file with LiveOcean points by study grid cell
LiveOcean_grid <- read.csv("~/Dropbox/PelagicEcosystemFunction_21/LiveOcean_pts(grid).csv")
LiveOcean_grid <- LiveOcean_grid %>% dplyr::select(c(point, latitude, longitude, grid_id))
LiveOcean_grid <- LiveOcean_grid %>% rename(
  lat = latitude,
  lon = longitude
)
```

Use points from previous chunk to extract data from NetCDF arrays:
```{r LiveOcean Extraction: PHYTOPLANKTON, echo=FALSE}
r_brick <- brick(phyto.array, xmn=min(lat), xmx=max(lat), ymn=min(lon), ymx=max(lon), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
# correct the orrientation of the raster brick
r_brick <- flip(t(r_brick), direction='y')

# extracting points from the brick
phyto_2017 <- data.frame()
phyto_2018 <- data.frame()
phyto_2019 <- data.frame()
phyto_2020 <- data.frame()
phyto_2021 <- data.frame()

for(i in 1:nrow(extraction_points)) {
  point_series <- raster::extract(r_brick, extraction_points[i,], method='simple')
  data <- data.frame(
    point = c(rep(i, times = 1764)),
    lat = rep(extraction_points[i,2], times = 1764), 
    lon = rep(extraction_points[i,1], times = 1764), 
    ocean_time = c(seq(1,1764)),
    phyto = c(point_series)
  )
  data17 <- data %>% filter(ocean_time >= 274 & ocean_time <= 334)
  data18 <- data %>% filter(ocean_time >= 639 & ocean_time <= 699)
  data19 <- data %>% filter(ocean_time >= 1004 & ocean_time <= 1064)
  data20 <- data %>% filter(ocean_time >= 1370 & ocean_time <= 1430)
  data21 <- data %>% filter(ocean_time >= 1735 & ocean_time <= 1764)
  phyto_2017 <- rbind(phyto_2017, data17)
  phyto_2018 <- rbind(phyto_2018, data18)
  phyto_2019 <- rbind(phyto_2019, data19)
  phyto_2020 <- rbind(phyto_2020, data20)
  phyto_2021 <- rbind(phyto_2021, data21)
  rm(data, data17, data18, data21, data19, data20)
  print(i)
}

# cleanup and formatting
phyto_2017 <- phyto_2017 %>% filter(!is.na(phyto))
phyto_2018 <- phyto_2018 %>% filter(!is.na(phyto))
phyto_2019 <- phyto_2019 %>% filter(!is.na(phyto))
phyto_2020 <- phyto_2020 %>% filter(!is.na(phyto))
phyto_2021 <- phyto_2021 %>% filter(!is.na(phyto))

phyto_2017 <- inner_join(phyto_2017, LiveOcean_grid, by = "point")
phyto_2018 <- inner_join(phyto_2018, LiveOcean_grid, by = "point")
phyto_2019 <- inner_join(phyto_2019, LiveOcean_grid, by = "point")
phyto_2020 <- inner_join(phyto_2020, LiveOcean_grid, by = "point")
phyto_2021 <- inner_join(phyto_2021, LiveOcean_grid, by = "point")

phyto_2017 <- phyto_2017 %>% filter(!is.na(grid_id))
phyto_2018 <- phyto_2018 %>% filter(!is.na(grid_id))
phyto_2019 <- phyto_2019 %>% filter(!is.na(grid_id))
phyto_2020 <- phyto_2020 %>% filter(!is.na(grid_id))
phyto_2021 <- phyto_2021 %>% filter(!is.na(grid_id))

rm(r_brick) #cleanup
```

```{r LiveOcean Extraction: SEA SURFACE TEMPERATURE, echo = FALSE}
temp_brick <- brick(temp.array, xmn=min(lat), xmx=max(lat), ymn=min(lon), ymx=max(lon), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
# correct the orrientation of the raster brick
temp_brick <- flip(t(temp_brick), direction='y')

temp_2017 <- data.frame()
temp_2018 <- data.frame()
temp_2019 <- data.frame()
temp_2020 <- data.frame()
temp_2021 <- data.frame()

for(i in 1:nrow(extraction_points)) {
  point_series <- raster::extract(temp_brick, extraction_points[i,], method='simple')
  data <- data.frame(
    point = c(rep(i, times = 1764)),
    lat = rep(extraction_points[i,2], times = 1764), 
    lon = rep(extraction_points[i,1], times = 1764), 
    ocean_time = c(seq(1,1764)),
    temp = c(point_series)
  )
  data17 <- data %>% filter(ocean_time >= 274 & ocean_time <= 334)
  data18 <- data %>% filter(ocean_time >= 639 & ocean_time <= 699)
  data19 <- data %>% filter(ocean_time >= 1004 & ocean_time <= 1064)
  data20 <- data %>% filter(ocean_time >= 1370 & ocean_time <= 1430)
  data21 <- data %>% filter(ocean_time >= 1735 & ocean_time <= 1764)
  temp_2017 <- rbind(temp_2017, data17)
  temp_2018 <- rbind(temp_2018, data18)
  temp_2019 <- rbind(temp_2019, data19)
  temp_2020 <- rbind(temp_2020, data20)
  temp_2021 <- rbind(temp_2021, data21)
  rm(data, data17, data18, data21, data19, data20)
  print(i)
}

# cleanup and formatting
temp_2017 <- temp_2017 %>% filter(!is.na(temp))
temp_2018 <- temp_2018 %>% filter(!is.na(temp))
temp_2019 <- temp_2019 %>% filter(!is.na(temp))
temp_2020 <- temp_2020 %>% filter(!is.na(temp))
temp_2021 <- temp_2021 %>% filter(!is.na(temp))

temp_2017 <- inner_join(temp_2017, LiveOcean_grid, by = "point")
temp_2018 <- inner_join(temp_2018, LiveOcean_grid, by = "point")
temp_2019 <- inner_join(temp_2019, LiveOcean_grid, by = "point")
temp_2020 <- inner_join(temp_2020, LiveOcean_grid, by = "point")
temp_2021 <- inner_join(temp_2021, LiveOcean_grid, by = "point")

temp_2017 <- temp_2017 %>% filter(!is.na(grid_id))
temp_2018 <- temp_2018 %>% filter(!is.na(grid_id))
temp_2019 <- temp_2019 %>% filter(!is.na(grid_id))
temp_2020 <- temp_2020 %>% filter(!is.na(grid_id))
temp_2021 <- temp_2021 %>% filter(!is.na(grid_id))

rm(temp_brick)
```

```{r LiveOcean Extraction: SALINITY}
## Data Extraction - SALINITY ##
sal_brick <- brick(sal.array, xmn=min(lat), xmx=max(lat), ymn=min(lon), ymx=max(lon), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
# correct the orrientation of the raster brick
sal_brick <- flip(t(sal_brick), direction='y')

sal_2017 <- data.frame()
sal_2018 <- data.frame()
sal_2019 <- data.frame()
sal_2020 <- data.frame()
sal_2021 <- data.frame()

for(i in 1:nrow(extraction_points)) {
  point_series <- raster::extract(sal_brick, extraction_points[i,], method='simple')
  data <- data.frame(
    point = c(rep(i, times = 1764)),
    lat = rep(extraction_points[i,2], times = 1764), 
    lon = rep(extraction_points[i,1], times = 1764), 
    ocean_time = c(seq(1,1764)),
    salt = c(point_series)
  )
  data17 <- data %>% filter(ocean_time >= 274 & ocean_time <= 334)
  data18 <- data %>% filter(ocean_time >= 639 & ocean_time <= 699)
  data19 <- data %>% filter(ocean_time >= 1004 & ocean_time <= 1064)
  data20 <- data %>% filter(ocean_time >= 1370 & ocean_time <= 1430)
  data21 <- data %>% filter(ocean_time >= 1735 & ocean_time <= 1764)
  sal_2017 <- rbind(sal_2017, data17)
  sal_2018 <- rbind(sal_2018, data18)
  sal_2019 <- rbind(sal_2019, data19)
  sal_2020 <- rbind(sal_2020, data20)
  sal_2021 <- rbind(sal_2021, data21)
  rm(data, data17, data18, data21, data19, data20)
  print(i)
}

# cleanup and formatting
sal_2017 <- sal_2017 %>% filter(!is.na(salt))
sal_2018 <- sal_2018 %>% filter(!is.na(salt))
sal_2019 <- sal_2019 %>% filter(!is.na(salt))
sal_2020 <- sal_2020 %>% filter(!is.na(salt))
sal_2021 <- sal_2021 %>% filter(!is.na(salt))

sal_2017 <- inner_join(sal_2017, LiveOcean_grid, by = "point")
sal_2018 <- inner_join(sal_2018, LiveOcean_grid, by = "point")
sal_2019 <- inner_join(sal_2019, LiveOcean_grid, by = "point")
sal_2020 <- inner_join(sal_2020, LiveOcean_grid, by = "point")
sal_2021 <- inner_join(sal_2021, LiveOcean_grid, by = "point")

sal_2017 <- sal_2017 %>% filter(!is.na(grid_id))
sal_2018 <- sal_2018 %>% filter(!is.na(grid_id))
sal_2019 <- sal_2019 %>% filter(!is.na(grid_id))
sal_2020 <- sal_2020 %>% filter(!is.na(grid_id))
sal_2021 <- sal_2021 %>% filter(!is.na(grid_id))

rm(sal_brick)
```

# Compile Data on Cruise Dates for Regressions
Compile the daily data for each variable, specifically for dates on which marine bird and mammal transects were conducted
```{r Create cumulative LiveOcean files}
phyto <- rbind(phyto_2017, phyto_2018, phyto_2019, phyto_2020, phyto_2021)
temp <- rbind(temp_2017, temp_2018, temp_2019, temp_2020, temp_2021)
salt <- rbind(sal_2017, sal_2018, sal_2019, sal_2020, sal_2021)
```

```{r Cruise Data, echo=FALSE}
# determine what grid cells fall within transect bounds
transect_zones <- read_csv("~/Dropbox/PelagicEcosystemFunction_21/transect_area.csv")
transect_zones <- transect_zones %>% dplyr::select(id, name)
transect_zones <- transect_zones %>% rename(
  grid_id = id,
  transect = name
)

cruises_ocean_time_all <- data.frame(
  Date = c("2017-10-03", "2017-10-10", "2017-10-24", "2017-10-31", "2017-11-07", "2017-11-16", "2018-10-02", "2018-10-11", "2018-10-16", "2018-10-23", "2018-10-30", "2018-11-06", "2019-10-02", "2019-10-19", "2019-10-23", "2019-10-30", "2019-11-05", "2020-10-06", "2020-10-07", "2020-10-15", "2020-10-20", "2020-10-26", "2020-11-02", "2020-11-10", "2021-10-07", "2021-10-12", "2021-10-19", "2021-10-26"),
  ocean_time = c(276, 283, 297, 304, 311, 320, 640, 649, 654, 661, 668, 675, 1005, 1022, 1026, 1033, 1034, 1375, 1376, 1384, 1389, 1395, 1402, 1410, 1741, 1746, 1753, 1760),
  cruise = c(seq(1,28))
)

cruises_ocean_time_all$Date <- as.Date(cruises_ocean_time_all$Date)

LO_phyto <- left_join(phyto, cruises_ocean_time_all)
LO_temp <- left_join(temp, cruises_ocean_time_all)
LO_salt <- left_join(salt, cruises_ocean_time_all)

LO_phyto <- LO_phyto %>% filter(!is.na(cruise))
LO_temp <- LO_temp %>% filter(!is.na(cruise))
LO_salt <- LO_salt %>% filter(!is.na(cruise))

#E summarize by grid cell ##
# Phytoplankton
LO_phyto <- LO_phyto %>% group_by(Date, grid_id) %>% summarise(
  phyto = mean(phyto, na.rm = TRUE)
)

# Temperature
sd <- LO_temp %>% group_by(Date, grid_id) %>% summarise(
  temp_sd = sd(temp, na.rm = TRUE)
)

LO_temp <- LO_temp %>% group_by(Date, grid_id) %>% summarise(
  temp = mean(temp, na.rm = TRUE)
)

LO_temp$temp_sd <- c(sd$temp_sd)
# Salinity
LO_salt <- LO_salt %>% group_by(Date, grid_id) %>% summarise(
  salt = mean(salt, na.rm = TRUE)
)

# PHYTOPLANKTON
## select data on transects
LO_phyto <- left_join(LO_phyto, transect_zones) # assign zones to each grid cell
LO_phyto <- LO_phyto %>% filter(!is.na(transect)) # filter out grid cells with no zones
# take the average daily value in each zone
LO_phyto <- LO_phyto %>% group_by(Date, transect) %>% summarise(
  phyto = mean(phyto)
)

LO_phyto <- LO_phyto %>% rename(
  Zone = transect
)
# change zone to factor
LO_phyto$Zone <- as.factor(LO_phyto$Zone)
# change Date to character
LO_phyto$Date <- as.character(LO_phyto$Date)
# SEA SURFACE TEMPERATRUE
## select data on transects
LO_temp <- left_join(LO_temp, transect_zones) # assign zones to each grid cell
LO_temp <- LO_temp %>% filter(!is.na(transect)) # filter out grid cells with no zones
# take the average daily value in each zone
LO_temp <- LO_temp %>% group_by(Date, transect) %>% summarise(
  temp = mean(temp),
  temp_sd = mean(temp_sd, na.rm = TRUE)
)

LO_temp <- LO_temp %>% rename(
  Zone = transect
)
# change zone to factor
LO_temp$Zone <- as.factor(LO_temp$Zone)
# change Date to character
LO_temp$Date <- as.character(LO_temp$Date)
# SALINITY
## select data on transects
LO_salt <- left_join(LO_salt, transect_zones) # assign zones to each grid cell
LO_salt <- LO_salt %>% filter(!is.na(transect)) # filter out grid cells with no zones
# take the average daily value in each zone
LO_salt <- LO_salt %>% group_by(Date, transect) %>% summarise(
  salt = mean(salt)
)

LO_salt <- LO_salt %>% rename(
  Zone = transect
)
# change zone to factor
LO_salt$Zone <- as.factor(LO_salt$Zone)
# change Date to character
LO_salt$Date <- as.character(LO_salt$Date)
```

# Compute Daily Values for Each Variable
Summarise the average value of each variable by grid cell and date:

```{r Summarizing LiveOcean by Date and Grid_ID 2017-2020, echo=FALSE}
phyto$Date <- as.Date(phyto$ocean_time, origin = "2016-12-31")
temp$Date <- as.Date(temp$ocean_time, origin = "2016-12-31")
salt$Date <- as.Date(salt$ocean_time, origin = "2016-12-31")

phyto <- phyto %>% group_by(Date, grid_id) %>% summarise(phyto = mean(phyto))
st.dv <- temp %>% group_by(Date, grid_id) %>% summarise(temp_sd = sd(temp, na.rm = TRUE))
temp <- temp %>% group_by(Date, grid_id) %>% summarise(temp = mean(temp))
temp$temp_sd <- st.dv$temp_sd
salt <- salt %>% group_by(Date, grid_id) %>% summarise(salt = mean(salt))
```

## COMPILE STUDY AREA AND ENVIRONMENTAL DATA
# Get static variable data from QGIS 
Read in the CSV file containing the static variables bathymetry, topography, shore distance, channel width, and tidal current as calculated in QGIS and stored in a local CSV:

```{r Static variable base file, echo=FALSE}
base <- read_csv("~/Dropbox/PelagicEcosystemFunction_21/sja_grid.csv")

# add channel width variable
channel.width <- read_csv("~/Dropbox/PelagicEcosystemFunction_21/channel_width.csv")
base <- inner_join(base, channel.width)

# make sure zonal statistic bins match grid bins
all.equal(base$left_bound, base$`Zonal Statistics_left`, check.attributes = TRUE)

# select columns of interest
base <- base %>% rename(
  NS_name = name,
  EW_name = `EW Joined_name`,
  NS_width = `length(m)`,
  EW_width = `EW Joined_length(m)`
)
base <- base %>% dplyr::select(grid_id, lat_deg, long_deg, centroid_depth, Station, average_depth, bathy__stdev, shore_distance, NS_width, EW_width)

# eliminate overland points
base <- base %>% filter(centroid_depth <= 0 | average_depth <= 0)
base <- base %>% filter(!is.na(centroid_depth)) 
base <- base[-c(444),]

# determine single channel width
ns <- base %>% filter((NS_width - EW_width) < 0 | is.na(EW_width))
ew <- base %>% filter((NS_width - EW_width) > 0 | is.na(NS_width))

# get rid of EW for NS channels
ns <- ns %>% dplyr::select(grid_id, NS_width)
ns <- ns %>% rename(
  channel_width = NS_width
)
# get rid of NS for EW channels
ew <- ew %>% dplyr::select(grid_id, EW_width)
ew <- ew %>% rename(
  channel_width = EW_width
)
width <- rbind(ns,ew)
base <- inner_join(base, width, by = "grid_id")
base <- base %>% dplyr::select(-c(NS_width, EW_width))

# cleanup
rm(ns, ew, width)

# add static tidal current proxy
base <- inner_join(base, tcur, by = "Station")

# rename using variable codes
base <- base %>% rename(
  bathy = average_depth,
  topog = bathy__stdev,
  dist = shore_distance,
  channel_width = channel_width
)
```

# Include dynamic variables from LiveOcean
Now include the daily values for each of my dynamic variables obtained from LiveOcean model; phytoplankton concentration, sea surface temperature, SST standard deviation, and salinity:

```{r Dynamic Variables}
env_all <- right_join(base, phyto, by = "grid_id")
env_all <- inner_join(env_all, temp, by = c("grid_id", "Date"))
env_all <- inner_join(env_all, salt, by = c("grid_id", "Date"))

env_all <- env_all %>% filter(!is.na(tcur))
```

## EXTRACTING MARINE BIRD AND MAMMAL DENSITIES
Here I extract the historical MBM density data by zone from the provided master sheet:
```{r MBM counts}
# read in MBM master data
MBM_master <- read_excel("~/Desktop/PEF/MBM_data/MBM_Data2008-2021_MASTER.xlsx", range = "countbyrecord!A1:AN30000")
MBM_master <- MBM_master %>% dplyr::select("Year", "Date", "Time", "Zone", "Transect", "GL", "CoMu", "HPorp", "HSeal")
# standardize time and date and year
MBM_master$Time <- hms::as_hms(MBM_master$Time)
MBM_master$Date <- ymd(MBM_master$Date)
MBM_master$Year <- year(MBM_master$Date)
# isolate years of interest
MBM_master <- MBM_master %>% filter(Year >= 2017)
MBM_master <- MBM_master %>% filter(Date != "2021-11-2" & Date != "2021-11-11")
# count observed individuals for each species by cruise date, zone, and transect (N/S vs. S/N)
MBM_master <- MBM_master %>% group_by(Year, Date, Zone, Transect) %>% summarize(GL = sum(GL), CoMu = sum(CoMu), HPorp = sum(HPorp), HSeal = sum(HSeal))
# Add in survey effort column 
single_transect_effort <- zone_effort
single_transect_effort <- single_transect_effort %>% mutate(effort = sqr_km / 2)
single_transect_effort <- single_transect_effort %>% dplyr::select(-c("sqr_km"))

MBM_master <- left_join(MBM_master, single_transect_effort, by = "Zone")
MBM_master$Zone <- factor(MBM_master$Zone)
# Total count and effort within each zone on each cruise day 
MBM_master <- ungroup(MBM_master)
MBM_master <- MBM_master %>% group_by(Date, Zone) %>% summarize (GL = sum(GL), CoMu = sum(CoMu), HPorp = sum(HPorp), HSeal = sum(HSeal), effort = sum(effort))

# reformat
GL <- MBM_master %>% dplyr::select(-c("CoMu", "HPorp", "HSeal"))
GL <- GL %>% rename(count = GL)
GL$species <- "GL"

CoMu <- MBM_master %>% dplyr::select(-c("GL", "HPorp", "HSeal"))
CoMu <- CoMu %>% rename(count = CoMu)
CoMu$species <- "CoMu"

HPorp <- MBM_master %>% dplyr::select(-c("GL", "CoMu", "HSeal"))
HPorp <- HPorp %>% rename(count = HPorp)
HPorp$species <- "HPorp"

HSeal <- MBM_master %>% dplyr::select(-c("GL", "CoMu", "HPorp"))
HSeal <- HSeal %>% rename(count = HSeal)
HSeal$species <- "HSeal"

MBM_master <- rbind(GL, CoMu, HPorp, HSeal)
```

# Mike's version
```{r Mike's version}
# read information on species codes, names and groups
species_names <- read_excel("data/MBM_data/MBM_Data2008-2021_MASTER.xlsx", range = "metadata!A10:f45")

# read data
byrecord <- read_excel("~/Desktop/PEF/MBM_data/MBM_Data2008-2021_MASTER.xlsx", range = "countbyrecord!A1:AN30000")

# select rows with data
byrecord %>% 
  filter(Date>0) ->  # remove blank lines
  byrecord           

# standardize time and date and year
byrecord$Time <- hms::as_hms(byrecord$Time)
byrecord$Date <- ymd(byrecord$Date)
byrecord$Year <- year(byrecord$Date)

### Sampling effort first ###
# read sampling effort (number of times a zone is sampled during a cruise) from a file
effort <- read_excel("MBM_Data2008-2021_MASTER.xlsx", range = "sampling effort!A14:g150")
effort$Date <- ymd(effort$Date)  # date format
# select rows with data
effort %>% 
  filter(Date>0) ->  # remove blank lines
  effort             # n = 89 records

# read area sizes (square kilometers) for each zone
area_size <- read_excel("MBM_Data2008-2021_MASTER.xlsx", range = "sampling effort!A2:b8")

# create tibble to translate zone naming, e.g., Zone_1 to 1
zone_names <- 
  tibble(
  Zone_name = c("Zone_1","Zone_2","Zone_3","Zone_4","Zone_5","Zone_6"), 
  Zone = seq(1,6)
)
  
# reformat effort 
effort %>%
  gather(key=Zone_name,value=Count,"Zone_1":"Zone_6") %>%
  left_join(zone_names,by = "Zone_name") %>%
  left_join(area_size,by = "Zone") ->  
  effort_long                                 # n = 534 records 

# compute area sampled for each date and zone
effort_long$Effort_sqkm <- effort_long$Count * effort_long$Size_sqkm

### Number of MBM is next ###
# sum species counts by Date and Zone 
byrecord %>%
  dplyr::select(-c(Year)) %>%                   # drop year from summing
  group_by(Date,Zone) %>%                       # group by date and zone
  summarize_if(is.numeric, sum, na.rm=TRUE) ->  # sum by date
byrecord_sumbyzone                              # n = 532 records 

# join effort data and species count data by Date and Zone
effort_long %>%
  left_join(byrecord_sumbyzone, by = c("Date","Zone")) %>%
  dplyr::select(c("Date","Zone","Effort_sqkm","GL":"RivOtt")) %>%
    gather(key=Species_code,value=Count,"GL":"RivOtt") ->
  mydata_bydatezone   # n = 19,950

# convert NA Count to 0 (these are sampled zones with no MBM observed)
mydata_bydatezone$Count[is.na(mydata_bydatezone$Count)] <- 0

# remove unsampled zones (Effort = 0)
mydata_bydatezone %>%
  dplyr::filter(Effort_sqkm > 0) ->
  mydata_bydatezone

# compute species density by Date and Zone
mydata_bydatezone$Density <- round(mydata_bydatezone$Count / mydata_bydatezone$Effort_sqkm,2)
```

# compare my calculations with Mike's
```{r MBM master comparison}
mydata_bydatezone$Zone <- factor(mydata_bydatezone$Zone)
test <- inner_join(MBM_master, mydata_bydatezone)
sum(test$count == test$Count) # counts are the same in all rows - excellent
sum(test$effort == test$Effort_sqkm) # there are four instances when effort is not the same in the two records

test_noag <- test %>% filter(effort != Effort_sqkm)
# NOTE: FOR DENSITY CALCULATIONS - USE EITHER COUNT BUT SHOULD USE MIKE'S EFFORT METRIC SINCE IT WAS OBTAINED FROM DIRECT NOTES LEFT IN THE DATA
MBM_master <- test[,-c(4,7,9)]
rm(test, test_noag)
```

## ZONE COMPARISONS
The first step of my analysis should be to compare the differences in density and frequency of presence between the different transect zones for each species to determine if they are significantly different. Because our data do not meet the assumption of normality for an ANOVA, I will apply a Kruskall-Wallis test instead. 

```{r Zone analysis of variance}
GL.kw <- mydata_bydatezone %>% filter(species == "GL")
# visualize the data
ggplot(data = GL.kw, aes(x = Zone, y = Density, color = Zone)) +
  geom_jitter(alpha = 0.5) +
  geom_boxplot(color = "black", fill = NA) +
  annotate(geom = "text", x = 1, y = 100, label = "a", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 2, y = 100, label = "a", color = "grey22", fontface = "italic") +
  geom_rect(aes(xmin = 0.5, xmax = 2.45, ymin = 0, ymax = 120), fill = NA, color = "grey", alpha = 0.6) +
  annotate(geom = "text", x = 3, y = 100, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 4, y = 100, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 5, y = 100, label = "c", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 6, y = 160, label = "b", color = "grey22", fontface = "italic") +
  geom_rect(aes(xmin = 4.55, xmax = 5.45, ymin = 0, ymax = 170), fill = NA, color = "grey", alpha = 0.6) +
  ylab("Density (indv./km2)") +
  facet_grid(species~., switch = "y") +
  theme_classic() + 
  theme(legend.position = "bottom") 
# run an Kruskall-Wallis test on the data
print(kruskal.test(Density ~ Zone, data = GL.kw)) # there are highly significant differences between the zones
# post-hoc testing
pairwise.wilcox.test(GL.kw$Density, GL.kw$Zone,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

# CoMu
CM <- mydata_bydatezone %>% filter(species == "CoMu")
# visualize the data
ggplot(data = CM, aes(x = Zone, y = Density, color = Zone)) +
  geom_jitter(alpha = 0.5) +
  geom_boxplot(color = "black", fill = NA) +
  annotate(geom = "text", x = 1, y = 250, label = "a", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 2, y = 250, label = "a", color = "grey22", fontface = "italic") +
  geom_rect(aes(xmin = 0.5, xmax = 2.45, ymin = 0, ymax = 270), fill = NA, color = "grey", alpha = 0.6) +
  annotate(geom = "text", x = 3, y = 750, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 4, y = 750, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 5, y = 750, label = "b", color = "grey22", fontface = "italic") +
  geom_rect(aes(xmin = 2.55, xmax = 5.45, ymin = 0, ymax = 780), fill = NA, color = "grey", alpha = 0.6) +
  annotate(geom = "text", x = 6, y = 1000, label = "c", color = "grey22", fontface = "italic")+
  geom_rect(aes(xmin = 5.55, xmax = 6.5, ymin = 0, ymax = 1020), fill = NA, color = "grey", alpha = 0.6) +
  ylab("Density (indv./km2)") +
  facet_grid(species~.) +
  theme_classic() + 
  theme(legend.position = "bottom")
# run an anova on the data
kruskal.test(Density ~ Zone, data = CM) # there are highly significant differences between the zones
# post-hoc testing
pairwise.wilcox.test(CM$Density, CM$Zone,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

# HPorp
HPorp <- mydata_bydatezone %>% filter(species == "HPorp")
# visualize the data
ggplot(data = HPorp, aes(x = Zone, y = Density, color = Zone)) +
  geom_jitter(alpha = 0.5) +
  geom_boxplot(color = "black", fill = NA) +
  annotate(geom = "text", x = 1, y = 10, label = "a", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 2, y = 10, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 3, y = 20, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 4, y = 20, label = "b", color = "grey22", fontface = "italic") +
  geom_rect(aes(xmin = 1.55, xmax = 4.45, ymin = 0, ymax = 25), fill = NA, color = "grey", alpha = 0.6) +
  annotate(geom = "text", x = 5, y = 10, label = "a", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 6, y = 10, label = "a", color = "grey22", fontface = "italic") +
  ylab("Density (indv./km2)") +
  facet_grid(species~.) +
  theme_classic() + 
  theme(legend.position = "bottom")
# run an anova on the data
kruskal.test(Density ~ Zone, data = HPorp) # there are highly significant differences between the zones

# post-hoc testing
pairwise.wilcox.test(HPorp$Density, HPorp$Zone,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

# HSeal
HSeal <- mydata_bydatezone %>% filter(species == "HSeal")
# visualize the data
ggplot(data = HSeal, aes(x = Zone, y = Density, color = Zone)) +
  geom_boxplot(color = "black", fill = NA) +
  geom_jitter(alpha = 0.5) +
  annotate(geom = "text", x = 1, y = 15, label = "a", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 2, y = 15, label = "a", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 3, y = 30, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 4, y = 20, label = "b", color = "grey22", fontface = "italic") +
  annotate(geom = "text", x = 5, y = 20, label = "b", color = "grey22", fontface = "italic") +
  geom_rect(aes(xmin = 2.55, xmax = 5.45, ymin = 0, ymax = 35), fill = NA, color = "grey", alpha = 0.6) +
  annotate(geom = "text", x = 6, y = 15, label = "a", color = "grey22", fontface = "italic") +
  ylab("Density (indv./km2)") +
  facet_grid(species~., switch = "y") +
  theme_classic() + 
  theme(legend.position = "bottom") 
# run an anova on the data 
kruskal.test(Density ~ Zone, data = HSeal) # there are highly significant differences between the zones
# post-hoc testing
pairwise.wilcox.test(HSeal$Density, HSeal$Zone,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

```

It's logical to assume that long-term differences in zonal abundance are primarily driven by static variables which are stable over decadal time scales. Let's see how my different static environmental variables vary across the different zones

Visually explore potential drivers of zonal abundance trends for seabirds:
```{r static environmental variables}
# add environmental data
static <- left_join(GL, trans_all)
static <- static %>% filter(!is.na(bathy))

## now let's examine variables which have historically been incorporated into the glaucous gull models
# bathymetry
ggplot() +
  geom_boxplot(data = GL, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_point(data = static, aes(x = Zone, y = -bathy), color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~., name = "Depth (m)")) +
  theme_classic()
# channel width
ggplot() +
  geom_boxplot(data = GL, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_point(data = static, aes(x = Zone, y = (channel_width / 150)), color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~.*150, name = "Channel Width (m)")) +
  theme_classic()
# tidal current amplitude
ggplot() +
  geom_boxplot(data = GL, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_point(data = static, aes(x = Zone, y = (tcur * 10)), color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~. / 10, name = "Tidal Current (knots)")) +
  theme_classic()
# sea-surface temperature
ggplot() +
  geom_boxplot(data = GL, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_boxplot(data = static, aes(x = Zone, y = temp*5), fill = NA, color = "black") +
  geom_jitter(data = static, aes(x = Zone, y = temp*5), alpha = 0.4, color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~./5, name = "SST (ºC)")) +
  theme_classic()
# sea-surface temperature standard deviation
ggplot() +
  geom_boxplot(data = GL, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_boxplot(data = static, aes(x = Zone, y = temp_sd*3000), fill = NA, color = "black") +
  geom_jitter(data = static, aes(x = Zone, y = temp_sd*3000), alpha = 0.4, color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~./3000, name = "SST SD(ºC)")) +
  theme_classic()
# sea-surface salinity
ggplot() +
  geom_boxplot(data = GL, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_boxplot(data = static, aes(x = Zone, y = salt*2), fill = NA, color = "black") +
  geom_jitter(data = static, aes(x = Zone, y = salt*2), alpha = 0.4, color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~./2, name = "Salinity(PSU)")) +
  theme_classic()
# phytoplankton concentration
ggplot() +
  geom_boxplot(data = GL, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_boxplot(data = static, aes(x = Zone, y = phyto*20), fill = NA, color = "black") +
  geom_jitter(data = static, aes(x = Zone, y = phyto*20), alpha = 0.4, color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~./20, name = "Phytoplankton (µmol/L)")) +
  theme_classic()

## COMMON MURRE
static <- left_join(CM, trans_all)
static <- static %>% filter(!is.na(bathy))

## now let's examine variables which have historically been incorporated into the glaucaous gull models
# distance from shore
ggplot() +
  geom_boxplot(data = CM, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_point(data = static, aes(x = Zone, y = dist / 5), color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~.* 5, name = "Distance from Shore (m)")) +
  theme_classic()
# bathymetry
ggplot() +
  geom_boxplot(data = CM, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_point(data = static, aes(x = Zone, y = -bathy*5), color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~./5, name = "Depth (m)")) +
  theme_classic()
# phytoplankton
ggplot() +
  geom_boxplot(data = CM, aes(x = Zone, y = Density, color = Zone), fill = NA) +
   geom_boxplot(data = static, aes(x = Zone, y = phyto*100), fill = NA, color = "black") +
  geom_jitter(data = static, aes(x = Zone, y = phyto*100), alpha = 0.4, color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~./100, name = "Depth (m)")) +
  theme_classic()
# tidal current amplitude
ggplot() +
  geom_boxplot(data = CM, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_point(data = static, aes(x = Zone, y = (tcur * 20)), color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~. / 20, name = "Tidal Current (knots)")) +
  theme_classic()
# sea-surface temperature standard deviation
ggplot() +
  geom_boxplot(data = CM, aes(x = Zone, y = Density, color = Zone), fill = NA) +
  geom_boxplot(data = static, aes(x = Zone, y = temp_sd*6000), fill = NA, color = "black") +
  geom_jitter(data = static, aes(x = Zone, y = temp_sd*6000), alpha = 0.4, color = "black") +
  scale_y_continuous(name = "Density", sec.axis = sec_axis(trans = ~./6000, name = "SST SD(ºC)")) +
  theme_classic()
```

Now, I will average the zonal densities and environmental conditions within each year and use generalized additive models to look for large-scale trends:

## INTERANNUAL TRENDS
This code chunk takes the annual average of both marine bird and mammal abundance and environmental conditions within each of the six zones. This is meant to compare how persistent interannual trends are influenced by persistent differences in environmental conditions and aims to match the temporal resolution of the training data with the patterns we aim to assess:

```{r Zonal Trend: Glaucous Gull}
## GLAUCOUS GULLS
static <- left_join(GL, trans_all)
static <- static %>% filter(!is.na(bathy))
static$dup <- static$Date
static <- static %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
static$Year <- factor(static$Year)
static <- static %>% filter(Year == 2017 | Year == 2018 | Year == 2019 | Year == 2020 | Year == 2021)
GL.static <- static %>% group_by(Year, Zone) %>% summarise(count = round(mean(Count),0), effort = mean(Effort_sqkm), bathy = mean(bathy), topog = mean(topog), dist = mean(dist), chwi = mean(channel_width), tcur = mean(tcur), phyto = mean(phyto), temp = mean(temp), temp_sd = mean(temp_sd), salt = mean(salt))
scaled_vars <- scale(GL.static[,5:13])
GL.static <- GL.static %>% dplyr::select(Year, Zone, count, effort)
GL.static <- cbind(GL.static, scaled_vars)
rm(scaled_vars)

lm.GL01 <- gam(count ~ s(bathy, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL02 <- gam(count ~ s(topog, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL03 <- gam(count ~ s(dist, k=4), family = poisson, offset = log(effort), data = GL.static)
#lm.GL04 <- gam(count ~ s(chwi, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL05 <- gam(count ~ s(tcur, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL06 <- gam(count ~ s(phyto, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL07 <- gam(count ~ s(temp, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL08 <- gam(count ~ s(temp_sd, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL09 <- gam(count ~ s(salt, k=4), family = poisson, offset = log(effort), data = GL.static)

# selecting the best model
vec_AIC <- AIC(lm.GL01, lm.GL02, lm.GL03, lm.GL04, lm.GL05, lm.GL06, lm.GL07, lm.GL08, lm.GL09)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.GL04)
rm(lm.GL01, lm.GL02, lm.GL03, lm.GL05, lm.GL06, lm.GL07, lm.GL08, lm.GL09)

# FS2
lm.GL1.01 <- gam(count ~ s(chwi, k=4) + s(bathy, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.02 <- gam(count ~ s(chwi, k=4) + s(temp, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.03 <- gam(count ~ s(chwi, k=4) + s(temp_sd, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.04 <- gam(count ~ s(chwi, k=4) + s(salt, k=4), family = poisson, offset = log(effort), data = GL.static)
# FS2.1
lm.GL1.01 <- gam(count ~ s(bathy, k=4) + s(topog, k=3), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.02 <- gam(count ~ s(bathy, k=4) + s(dist, k=3), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.03 <- gam(count ~ s(bathy, k=4) + s(phyto, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.04 <- gam(count ~ s(bathy, k=4) + s(temp, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.05 <- gam(count ~ s(bathy, k=4) + s(temp_sd, k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL1.06 <- gam(count ~ s(bathy, k=4) + s(salt, k=4), family = poisson, offset = log(effort), data = GL.static)

# selecting the best model
vec_AIC <- AIC(lm.GL01, lm.GL1.01, lm.GL1.02, lm.GL1.03, lm.GL1.04, lm.GL1.05, lm.GL1.06)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.GL1.02)
rm(lm.GL1.02, lm.GL1.03, lm.GL1.04)

#FS3
lm.GL11.01 <- gam(count ~ s(chwi, k=4) + s(bathy, k=3) + s(temp,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL11.02 <- gam(count ~ s(chwi, k=4) + s(bathy, k=4) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL11.03 <- gam(count ~ s(chwi, k=4) + s(bathy, k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = GL.static)
#FS3.1
lm.GL11.01 <- gam(count ~ s(bathy, k=3) + s(dist,k=3) + s(phyto,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL11.02 <- gam(count ~ s(bathy, k=3) + s(dist,k=3) + s(temp,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL11.03 <- gam(count ~ s(bathy, k=3) + s(dist,k=3) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL11.04 <- gam(count ~ s(bathy, k=3) + s(dist,k=3) + s(salt,k=4), family = poisson, offset = log(effort), data = GL.static)

# selecting the best model
vec_AIC <- AIC(lm.GL1.02, lm.GL11.01, lm.GL11.02, lm.GL11.03, lm.GL11.04)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.GL11.02)
rm(lm.GL11.02, lm.GL11.03)

#FS4
lm.GL111.01 <- gam(count ~ s(chwi, k=4) + s(bathy, k=4) + s(temp,k=4) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL111.02 <- gam(count ~ s(chwi, k=4) + s(bathy, k=4) + s(temp,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = GL.static)
#FS4.1
lm.GL111.01 <- gam(count ~ s(bathy, k=3) + s(dist,k=3) + s(temp,k=4) + s(phyto,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL111.02 <- gam(count ~ s(bathy, k=3) + s(dist,k=3) + s(temp,k=4) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = GL.static)
lm.GL111.03 <- gam(count ~ s(bathy, k=3) + s(dist,k=3) + s(temp,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = GL.static)

# selecting the best model
vec_AIC <- AIC(lm.GL11.02, lm.GL111.01, lm.GL111.02, lm.GL111.03)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw
# phytoplankton was the next best predictor 
summary(lm.GL111.01)
rm(lm.GL111.03, lm.GL111.02)

# plotting model results
# SST
newdata1 <- with(GL.static, data.frame(effort = mean(effort), bathy = mean(bathy), dist = mean(dist), temp = rep(seq(-2,2.5, 0.05), times = 1), phyto = mean(phyto))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.GL111.01, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
ggplot(newdata2) + 
  geom_ribbon(aes(x = temp, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(aes(x = temp, y = fit)) +
  geom_point(data = GL.static, aes(x = temp, y = count, color = Zone), alpha = 0.5) +
  xlab("SST (ºC)") +
  ylab("Glaucous Gull Count") +
  theme_classic() # gulls are more common in regions of higher surface temperatures

# SHORE DISTANCE
newdata1 <- with(GL.static, data.frame(effort = mean(effort), bathy = mean(bathy), dist = seq(-1,2.5, 0.05), temp = mean(temp), phyto = mean(phyto))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.GL111.01, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})

ggplot(data = newdata2) + 
  geom_point(data = GL.static, aes(x = dist, y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = dist, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = dist, y = fit)) +
  xlab("Shore Distance (m)") +
  ylab("Glaucous Gull Count") +
  theme_classic() # gulls appear to be more common in wider channels

# PHYTO
newdata1 <- with(GL.static, data.frame(effort = mean(effort), bathy = mean(bathy), dist = mean(dist), temp = mean(temp), phyto = seq(-2,2,0.05))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.GL111.01, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})

ggplot(data = newdata2) + 
  geom_point(data = GL.static, aes(x = phyto, y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = phyto, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = phyto, y = fit)) +
  xlab("[Phytoplankton] (µmol/L)") +
  ylab("Glaucous Gull Count") +
  theme_classic()

# DEPTH
newdata1 <- with(GL.static, data.frame(effort = mean(effort), bathy = seq(-2,2,0.05), dist = mean(dist), temp = mean(temp), phyto = mean(phyto))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.GL111.01, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})

ggplot(data = newdata2) + 
  geom_point(data = GL.static, aes(x = -bathy, y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = -bathy, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = -bathy, y = fit)) +
  xlab("Water Depth (m)") +
  ylab("Glaucous Gull Count") +
  theme_classic() # gulls are more common in shallower regions (like where shelves force aggregate water from the strait near the surface)
```

``` {r Zonal Trends: Common Murre}
## COMMON MURRE
static <- left_join(CM, trans_all)
static <- static %>% filter(!is.na(bathy))
static$dup <- static$Date
static <- static %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
static$Year <- factor(static$Year)
static <- static %>% filter(Year == 2017 | Year == 2018 | Year == 2019 | Year == 2020 | Year == 2021)
CM.static <- static %>% group_by(Year, Zone) %>% summarise(count = round(mean(Count),0), effort = mean(Effort_sqkm), bathy = mean(bathy), topog = mean(topog), dist = mean(dist), chwi = mean(channel_width), tcur = mean(tcur), phyto = mean(phyto), temp = mean(temp), temp_sd = mean(temp_sd), salt = mean(salt))
scaled_vars <- scale(CM.static[,5:13])
CM.static <- CM.static %>% dplyr::select(Year, Zone, count, effort)
CM.static <- cbind(CM.static, scaled_vars)
rm(scaled_vars)

lm.CM01 <- gam(count ~ s(bathy,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM02 <- gam(count ~ s(topog,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM03 <- gam(count ~ s(dist,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM04 <- gam(count ~ s(chwi,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM05 <- gam(count ~ s(tcur,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM06 <- gam(count ~ s(phyto,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM07 <- gam(count ~ s(temp,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM08 <- gam(count ~ s(temp_sd,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM09 <- gam(count ~ s(salt,k=4), family = poisson, offset = log(effort), data = CM.static)

# selecting the best model
vec_AIC <- AIC(lm.CM01, lm.CM02, lm.CM03, lm.CM04, lm.CM05, lm.CM06, lm.CM07, lm.CM08, lm.CM09)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.CM03)
rm(lm.CM01, lm.CM02, lm.CM04, lm.CM05, lm.CM06, lm.CM07, lm.CM08, lm.CM09)
# distance from shore is the best predictor of murre abundance

#FS2
lm.CM1.01 <- gam(count ~ s(dist,k=3) + s(bathy,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM1.02 <- gam(count ~ s(dist,k=3) + s(phyto,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM1.03 <- gam(count ~ s(dist,k=3) + s(temp,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM1.04 <- gam(count ~ s(dist,k=3) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM1.05 <- gam(count ~ s(dist,k=3) + s(salt,k=4), family = poisson, offset = log(effort), data = CM.static)

# selecting the best model
vec_AIC <- AIC(lm.CM03, lm.CM1.01, lm.CM1.02, lm.CM1.03, lm.CM1.04, lm.CM1.05)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.CM1.02)
rm(lm.CM1.01, lm.CM1.03, lm.CM1.04, lm.CM1.05, lm.CM1.06, lm.CM1.07, lm.CM1.08)
# phytoplankton is the next best predictor of murre abundance

#FS3
lm.CM11.01 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(bathy,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM11.02 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(temp,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM11.03 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM11.04 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = CM.static)

# selecting the best model
vec_AIC <- AIC(lm.CM1.02, lm.CM11.01, lm.CM11.02, lm.CM11.03, lm.CM11.04)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.CM11.03)
rm(lm.CM11.01, lm.CM11.02, lm.CM11.04)
# SST standard deviation is the next best predictor of murre abundance

#FS4
lm.CM111.01 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(temp_sd,k=4) + s(bathy,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM111.02 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(temp_sd,k=4) + s(temp,k=4), family = poisson, offset = log(effort), data = CM.static)
lm.CM111.03 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(temp_sd,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = CM.static)

# selecting the best model
vec_AIC <- AIC(lm.CM11.03, lm.CM111.01, lm.CM111.02, lm.CM111.03)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.CM111.02)
rm(lm.CM111.01, lm.CM111.03)
# temperature is the next best predictor 

# FS5
lm.CM1111.01 <- gam(count ~ s(dist,k=3) + s(phyto,k=4) + s(temp_sd,k=4) + s(temp,k=4) + s(bathy,k=3), family = poisson, offset = log(effort), data = CM.static)
lm.CM1111.02 <- gam(count ~ s(dist,k=3) + s(phyto,k=3) + s(temp_sd,k=4) + s(temp,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = CM.static)

# selecting the best model
vec_AIC <- AIC(lm.CM111.02, lm.CM1111.01, lm.CM1111.02)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.CM1111.02)
rm(lm.CM1111.01)
# salinity is the next best variable

# FS6
lm.CM11111.01 <- gam(count ~ s(dist,k=3) + s(phyto,k=3) + s(temp_sd,k=4) + s(temp,k=4) + s(salt,k=4) + s(bathy,k=3), family = poisson, offset = log(effort), data = CM.static)

# selecting the best model
vec_AIC <- AIC(lm.CM1111.02, lm.CM11111.01)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw
# this variable did not substantially improve the model
summary(lm.CM11111.01)

summary(lm.CM1111.02) # best model

## PLOTTING MODEL RESULTS: COMU
# DISTANCE FROM SHORE
newdata1 <- with(CM.static, data.frame(effort = mean(effort), dist = seq(-1,2.5, 0.05), phyto = mean(phyto), temp_sd = mean(temp_sd), temp = mean(temp), salt = mean(salt), bathy = mean(bathy))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.CM1111.02, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$dist)
newdata2$dist <- newdata2$dist * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = CM.static, aes(x = (dist * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = dist, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = dist, y = fit)) +
  xlab("Distance from Shore (m)") +
  ylab("Common Murre Count") +
  theme_classic() # murres are more common farther from shore

# PHYTOPLANKTON CONCENTRATION
newdata1 <- with(CM.static, data.frame(effort = mean(effort), dist = mean(dist), phyto = seq(-2,2,0.05), temp_sd = mean(temp_sd), temp = mean(temp), salt = mean(salt), bathy = mean(bathy))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.CM1111.02, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$phyto)
newdata2$phyto <- newdata2$phyto * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = CM.static, aes(x = (phyto * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = phyto, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = phyto, y = fit)) +
  xlab("Phytoplankton Concentration (µmol/L)") +
  ylab("Common Murre Count") +
  theme_classic() # phytoplankton concentration has little impact in water w/ low topography but in regions of sills / canyons increasing phytoplankton concentration dramatically increases murre abundance

# SST SD
newdata1 <- with(CM.static, data.frame(effort = mean(effort), dist = mean(dist), phyto = mean(phyto), temp_sd = seq(-2,2.5,0.05), temp = mean(temp), salt = mean(salt))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.CM1111.02, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$temp_sd)
newdata2$temp_sd <- newdata2$temp_sd * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = CM.static, aes(x = (temp_sd * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Year), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = temp_sd, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = temp_sd, y = fit)) +
  xlab("SST Standard Deviation (ºC)") +
  ylab("Common Murre Count") +
  theme_classic()
# SALINITY
newdata1 <- with(CM.static, data.frame(effort = mean(effort), dist = mean(dist), phyto = mean(phyto), temp_sd = mean(temp_sd), temp = mean(temp), salt = seq(-2.5,2,0.05))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.CM1111.02, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$salt)
newdata2$salt <- newdata2$salt * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = CM.static, aes(x = (salt * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Year), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = salt, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = salt, y = fit)) +
  xlab("SSS (PSU)") +
  ylab("Common Murre Count") +
  theme_classic()
# SST
newdata1 <- with(CM.static, data.frame(effort = mean(effort), dist = mean(dist), phyto = mean(phyto), temp_sd = mean(temp_sd), temp = seq(-2,2.5,0.05), salt = mean(salt))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.CM1111.02, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$temp)
newdata2$temp <- newdata2$temp * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = CM.static, aes(x = (temp * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Year), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = temp, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = temp, y = fit)) +
  xlab("SST (ºC)") +
  ylab("Common Murre Count") +
  theme_classic()
```

```{r Zonal Trend Drivers: Harbor Porpoise}
## HARBOR PORPOISE
static <- left_join(HPorp, trans_all)
static <- static %>% filter(!is.na(bathy))
static$dup <- static$Date
static <- static %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
static$Year <- factor(static$Year)
static <- static %>% filter(Year == 2017 | Year == 2018 | Year == 2019 | Year == 2020 | Year == 2021)
Present <- static %>% filter(Count != 0)
Present$PA <- as.numeric(1)
Absent <- static %>% filter(Count == 0)
Absent$PA <- as.numeric(0)
static <- rbind(Present, Absent)
rm(Present, Absent)
HP.static <- static %>% group_by(Year, Month, Zone) %>% summarise(count = round(mean(Count),0), effort = mean(Effort_sqkm), Density = mean(Density), PA = mean(PA), bathy = mean(bathy), topog = mean(topog), dist = mean(dist), chwi = mean(channel_width), tcur = mean(tcur), phyto = mean(phyto), temp = mean(temp), temp_sd = mean(temp_sd), salt = mean(salt))
scaled_vars <- scale(HP.static[,8:16])
HP.static <- HP.static %>% dplyr::select(Year, Zone, count, effort, Density, PA)
HP.static <- cbind(HP.static, scaled_vars)
rm(scaled_vars)

lm.HP01 <- gam(count ~ s(bathy, k=3), family = poisson, offset = log(effort), data = HP.static)
lm.HP02 <- gam(count ~ s(topog, k=3), family = poisson, offset = log(effort), data = HP.static)
lm.HP03 <- gam(count ~ s(dist, k=3), family = poisson, offset = log(effort), data = HP.static)
lm.HP04 <- gam(count ~ s(chwi, k=3), family = poisson, offset = log(effort), data = HP.static)
lm.HP05 <- gam(count ~ s(tcur, k=3), family = poisson, offset = log(effort), data = HP.static)
lm.HP06 <- gam(count ~ s(phyto, k=4), family = poisson, offset = log(effort), data = HP.static)
lm.HP07 <- gam(count ~ s(temp, k=4), family = poisson, offset = log(effort), data = HP.static)
lm.HP08 <- gam(count ~ s(temp_sd, k=4), family = poisson, offset = log(effort), data = HP.static)
lm.HP09 <- gam(count ~ s(salt, k=4), family = poisson, offset = log(effort), data = HP.static)
#NEGBINOM
lm.HP01 <- gam(Density ~ s(bathy, k=3), familiy = nb, data = HP.static)
lm.HP02 <- gam(Density ~ s(topog, k=3), familiy = nb , data = HP.static)
lm.HP03 <- gam(Density ~ s(dist, k=3), familiy = nb , data = HP.static)
lm.HP04 <- gam(Density ~ s(chwi, k=3), familiy = nb , data = HP.static)
lm.HP05 <- gam(Density ~ s(tcur, k=3), familiy = nb , data = HP.static)
lm.HP06 <- gam(Density ~ s(phyto, k=4), familiy = nb , data = HP.static) # this model is actually significant
lm.HP07 <- gam(Density ~ s(temp, k=4), familiy = nb , data = HP.static)
lm.HP08 <- gam(Density ~ s(temp_sd, k=4), familiy = nb, data = HP.static)
lm.HP09 <- gam(Density ~ s(salt, k=4), family = nb, data = HP.static)
# PA
lm.HP01 <- gam(PA ~ s(bathy, k=3), data = HP.static)
lm.HP02 <- gam( PA ~ s(topog, k=3) , data = HP.static)
lm.HP03 <- gam( PA ~ s(dist, k=3) , data = HP.static)
lm.HP04 <- gam( PA ~ s(chwi, k=3) , data = HP.static)
lm.HP05 <- gam( PA ~ s(tcur, k=3) , data = HP.static)
lm.HP06 <- gam( PA ~ s(phyto, k=4) , data = HP.static) # this model is actually significant
lm.HP07 <- gam( PA ~ s(temp, k=4) , data = HP.static)
lm.HP08 <- gam( PA ~ s(temp_sd, k=4) , data = HP.static)
lm.HP09 <- gam( PA ~ s(salt, k=4), data = HP.static)

# selecting the best model
vec_AIC <- AIC(lm.HP01, lm.HP02, lm.HP03, lm.HP04, lm.HP05, lm.HP06, lm.HP07, lm.HP08, lm.HP09)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.HP06)
rm(lm.HP02, lm.HP03, lm.HP04, lm.HP05, lm.HP07, lm.HP08, lm.HP09)

#FS2
lm.HP6.1 <- gam(PA ~ s(phyto, k=4) + s(bathy,k=3), data = HP.static) 
lm.HP6.2 <- gam(PA ~ s(phyto, k=4) + s(topog,k=3), data = HP.static) 
lm.HP6.3 <- gam(PA ~ s(phyto, k=4) + s(dist,k=3), data = HP.static) 
lm.HP6.4 <- gam(PA ~ s(phyto, k=4) + s(temp,k=4), data = HP.static) 
lm.HP6.5 <- gam(PA ~ s(phyto, k=4) + s(temp_sd,k=4), data = HP.static) 
lm.HP6.6 <- gam(PA ~ s(phyto, k=4) + s(salt,k=4), data = HP.static) 

# selecting the best model
vec_AIC <- AIC(lm.HP06, lm.HP6.1, lm.HP6.2, lm.HP6.3, lm.HP6.4, lm.HP6.5, lm.HP6.6)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.HP6.6)
rm(lm.HP6.1, lm.HP6.2, lm.HP6.3, lm.HP6.4, lm.HP6.5)

#FS3
lm.HP6.6.1 <- gam(PA ~ s(phyto, k=4) + s(salt,k=4) + s(bathy,k=3), data = HP.static)
lm.HP6.6.2 <- gam(PA ~ s(phyto, k=4) + s(salt,k=4) + s(topog,k=3), data = HP.static)
lm.HP6.6.3 <- gam(PA ~ s(phyto, k=4) + s(salt,k=4) + s(dist,k=3), data = HP.static)
lm.HP6.6.4 <- gam(PA ~ s(phyto, k=4) + s(salt,k=4) + s(temp,k=4), data = HP.static)
lm.HP6.6.5 <- gam(PA ~ s(phyto, k=4) + s(salt,k=4) + s(temp_sd,k=4), data = HP.static)

# selecting the best model
vec_AIC <- AIC(lm.HP6.6, lm.HP6.6.1, lm.HP6.6.2, lm.HP6.6.3, lm.HP6.6.4, lm.HP6.6.5)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.HP6.6.1)
summary(lm.HP6.6.4)
# this iteration slightly improved the model and added two nearly significant variables, I'll continue adding bathymetry for exploratory purposes

summary(lm.HP06) # best model

## PLOTTING MODEL RESULTS
# [PHYTOPLANKTON]
newdata1 <- with(HP.static, data.frame(effort = mean(effort), phyto = seq(-2,2, 0.05), salt = mean(salt), bathy = mean(bathy))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.HP06, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$phyto)
newdata2$phyto <- newdata2$phyto * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = HP.static, aes(x = (phyto * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = PA, color = Year), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = phyto, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = phyto, y = fit)) +
  xlab("[Phytoplankton] (µmol/L)") +
  ylab("Liklihood of Presence") +
  theme_classic() # porpoises become less common in highly productive years perhaps because they move to different foraging grounds farther from shore / have not returned from offshore distribution

# SURFACE SALINITY
newdata1 <- with(HP.static, data.frame(effort = mean(effort), phyto = mean(phyto), salt = seq(-2.5,1.5, 0.05), bathy = mean(bathy))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.HP6.6.1, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$salt)
newdata2$salt <- newdata2$salt * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = HP.static, aes(x =(salt* attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = PA, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = salt, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = salt, y = fit)) +
  xlab("Surface Salinity (PSU)") +
  ylab("Liklihood of Presence") +
  theme_classic()

# WATER DEPTH
newdata1 <- with(HP.static, data.frame(effort = mean(effort), phyto = mean(phyto), salt = seq(-2.5,1.5, 0.05), bathy = mean(bathy))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.HP6.6.1, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})

ggplot(data = newdata2) + 
  geom_point(data = HP.static, aes(x = salt, y = PA, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = salt, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = salt, y = fit)) +
  xlab("Surface Salinity (PSU)") +
  ylab("Liklihood of Presence") +
  theme_classic()

```

```{r Zonal Trend Drivers: Harbor Seal}
static <- left_join(HSeal, trans_all)
static <- static %>% filter(!is.na(bathy))
static$dup <- static$Date
static <- static %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
static$Year <- factor(static$Year)
static <- static %>% filter(Year == 2017 | Year == 2018 | Year == 2019 | Year == 2020 | Year == 2021)
Present <- static %>% filter(Count != 0)
Present$PA <- as.numeric(1)
Absent <- static %>% filter(Count == 0)
Absent$PA <- as.numeric(0)
static <- rbind(Present, Absent)
rm(Present, Absent)
HS.static <- static %>% group_by(Year, Zone) %>% summarise(count = round(mean(Count),0), effort = mean(Effort_sqkm), PA = mean(PA), bathy = mean(bathy), topog = mean(topog), dist = mean(dist), chwi = mean(channel_width), tcur = mean(tcur), phyto = mean(phyto), temp = mean(temp), temp_sd = mean(temp_sd), salt = mean(salt))
scaled_vars <- scale(HS.static[,6:14])
HS.static <- HS.static %>% dplyr::select(Year, Zone, count, effort, PA)
HS.static <- cbind(HS.static, scaled_vars)
rm(scaled_vars)

# PA
lm.HS01 <- gam(count ~ s(bathy,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS02 <- gam(count ~ s(topog,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS03 <- gam(count ~ s(dist,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS04 <- gam(count ~ s(chwi,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS05 <- gam(count ~ s(tcur,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS06 <- gam(count ~ s(phyto,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS07 <- gam(count ~ s(temp,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS08 <- gam(count ~ s(temp_sd,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS09 <- gam(count ~ s(salt,k=4), family = poisson, offset = log(effort), data = HS.static)

# selecting the best model
vec_AIC <- AIC(lm.HS01, lm.HS02, lm.HS03, lm.HS04, lm.HS05, lm.HS06, lm.HS07, lm.HS08, lm.HS09)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.HS01)
rm(lm.HS03, lm.HS02, lm.HS04, lm.HS05, lm.HS06, lm.HS07, lm.HS08, lm.HS09)

#FS2
lm.HS3.1 <- gam(count ~ s(bathy,k=3) + s(topog,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.2 <- gam(count ~ s(bathy,k=3) + s(dist,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.3 <- gam(count ~ s(bathy,k=3) + s(chwi,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.4 <- gam(count ~ s(bathy,k=3) + s(phyto,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.5 <- gam(count ~ s(bathy,k=3) + s(temp,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.6 <- gam(count ~ s(bathy,k=3) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.7 <- gam(count ~ s(bathy,k=3) + s(salt,k=4), family = poisson, offset = log(effort), data = HS.static)


# selecting the best model
vec_AIC <- AIC(lm.HS3.1, lm.HS3.2, lm.HS3.3, lm.HS3.4, lm.HS3.5, lm.HS3.6, lm.HS3.7)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.HS3.4)
rm(lm.HS3.2, lm.HS3.3, lm.HS3.1, lm.HS3.5, lm.HS3.6, lm.HS3.7)

#FS3
lm.HS3.1.1 <- gam(count ~ s(bathy,k=3) + s(phyto,k=4) + s(topog,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.1.2 <- gam(count ~ s(bathy,k=3) + s(phyto,k=4) + s(dist,k=3), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.1.3 <- gam(count ~ s(bathy,k=3) + s(phyto,k=4) + s(temp,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.1.4 <- gam(count ~ s(bathy,k=3) + s(phyto,k=4) + s(temp_sd,k=4), family = poisson, offset = log(effort), data = HS.static)
lm.HS3.1.5 <- gam(count ~ s(bathy,k=3) + s(phyto,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = HS.static)


# selecting the best model
vec_AIC <- AIC(lm.HS3.4, lm.HS3.1.1, lm.HS3.1.2, lm.HS3.1.3, lm.HS3.1.4, lm.HS3.1.5)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(lm.HS3.1.1)
summary(lm.HS3.1.2)
summary(lm.HS3.1.3)
rm(lm.HS3.1.1, lm.HS3.1.2, lm.HS3.1.3, lm.HS3.1.4, lm.HS3.1.5)
# this iteration introduced several nearly significant variables, topography has a 75% chance of producing a more accurate model and so I'll explore it

summary(lm.HS3.4) # best model
summary(lm.HS3.1.1) # (possible) best model

## PLOTTING MODEL RESULTS: HSeal
# WATER DEPTH
newdata1 <- with(HS.static, data.frame(effort = mean(effort), topog = mean(topog), bathy = seq(-2,1.5, 0.05), phyto = mean(phyto))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.HS3.4, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$bathy)
newdata2$bathy <- newdata2$bathy * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = HS.static, aes(x = -(bathy * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = -bathy, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = -bathy, y = fit)) +
  xlab("Water Depth (m)") +
  ylab("Predicted Count (#)") +
  theme_classic() # seals are more common in shallow water

# PHYTO
newdata1 <- with(HS.static, data.frame(effort = mean(effort), topog = mean(topog), bathy = mean(bathy), phyto = seq(-2,2, 0.05))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.HS3.4, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$phyto)
newdata2$phyto <- newdata2$phyto * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = HS.static, aes(x = (phyto * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = phyto, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = phyto, y = fit)) +
  xlab("[Phytoplankton] (µmol/L)") +
  ylab("Predicted Count (#)") +
  theme_classic() # seals are more common in shallow water

# topog
# PHYTO
newdata1 <- with(HS.static, data.frame(effort = mean(effort), topog = seq(-2,2, 0.05), bathy = mean(bathy), phyto = mean(phyto))) # choose variable to model
newdata2 <- cbind(newdata1, predict(lm.HS3.1.1, newdata = newdata1, type = "response", se = TRUE))
newdata2 <- within(newdata2, {
  LL <- fit - (1.96 * se.fit)
  UL <- fit + (1.96 * se.fit)
})
s.x <- scale(trans_c$topog)
newdata2$topog <- newdata2$topog * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata2) + 
  geom_point(data = HS.static, aes(x = (topog * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')), y = count, color = Zone), alpha = 0.5) +
  geom_ribbon(data = newdata2, aes(x = topog, y = fit, ymin = LL, ymax = UL), alpha = 0.1) + 
  geom_line(data = newdata2, aes(x = topog, y = fit)) +
  xlab("Bottom Topography (m)") +
  ylab("Predicted Count (#)") +
  theme_classic() # seals are more common in shallow water
```

# Examining for correlations
Lets explore to see if any of our environmental variables are cross correlated with eachother. Note: These correlations should be identical no matter which species' data frame I draw them from since the averaved environmental values within each zone are the same. 
```{r Chart Correlation}
mydata <- GL.static[,c(5:13)]

cor.test <- cor(mydata, use = "complete.obs")

chart.Correlation(mydata)
```

# Leave-one-year-out Validation
This chunk performs leave-one-year-out validation on course-scale habitat models: 
```{r LOO Validation}
# create a master dataframe with all observations
mydata <- GL.static[,c(1,2,4:13)]
mydata$GL <- GL.static$count
mydata$CM <- CM.static$count
mydata$HS <- HS.static$count
mydata$HP <- HP.static$PA

# loop that filters out each year, trains each species model, and computes/stores validation metrics
LYO_results <- data.frame()
LYO_raw <- data.frame()
for (y in unique(mydata$Year)) {
  ## create train and test sets
  counter = 1
  train <- mydata %>% filter(Year != y)
  test <- mydata %>% filter(Year == y)
  ## train a model for each species
  lm.HP <- gam(HP ~ s(phyto, k=4) , data = train)
  lm.HS <- gam(HS ~ s(bathy,k=3) + s(phyto,k=4) + s(topog,k=4), family = poisson, offset = log(effort), data = train)
  lm.CM <- gam(CM ~ s(dist,k=3) + s(phyto,k=3) + s(temp_sd,k=4) + s(temp,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = train)
  lm.GL <- gam(GL ~ s(bathy, k=3) + s(dist,k=3) + s(temp,k=4) + s(phyto,k=4), family = poisson, offset = log(effort), data = train)
  ## apply these models to test data
  test$HP.p <- predict(lm.HP, newdata = test, type = "response", se = FALSE)
  test$HS.p <- predict(lm.HS, newdata = test, type = "response", se = FALSE)
  test$CM.p <- predict(lm.CM, newdata = test, type = "response", se = FALSE)
  test$GL.p <- predict(lm.GL, newdata = test, type = "response", se = FALSE)

  ## compute and store validation metrics
  # prediction error and adj. r-squared
  test$HS[test$HS == 0] <- 0.001 # prevent infinite prediction error
  test$HP[test$HP == 0] <- 0.001 # prevent infinite prediction error
  test$CM[test$CM == 0] <- 0.001 # prevent infinite prediction error
  test$GL[test$GL == 0] <- 0.001 # prevent infinite prediction error
  test <- test %>% mutate(
    d.HP = (((HP - HP.p) / HP) * 100),
    d.HS = (((HS - HS.p) / HS) * 100),
    d.CM = (((CM - CM.p) / CM) * 100),
    d.GL = (((GL - GL.p) / GL) * 100)
  )
  data <- data.frame(
    year = y,
    Zone = rep(c(1,2,3,4,5,6), times = 4),
    species = rep(c("HP", "HS", "CM", "GL"), each = 6),
    R.squared = rep(c(summary(lm.HP)$r.sq, summary(lm.HS)$r.sq, summary(lm.CM)$r.sq, summary(lm.GL)$r.sq), each = 6),
    Dev.Expl = rep(c(summary(lm.HP)$dev.ex, summary(lm.HS)$dev.ex, summary(lm.CM)$dev.ex, summary(lm.GL)$dev.ex), each = 6)
  )
  
  LYO_raw <- rbind(LYO_raw, test[,c(1,2,13:20)])
  LYO_results <- rbind(LYO_results, data)
  print(y)
  rm(data, train, test, lm.HP, lm.HS, lm.CM, lm.GL)
}

# same process but for zones
LZO_results <- data.frame()
LZO_raw <- data.frame()
for (z in unique(mydata$Zone)) {
  ## create train and test sets
  train <- mydata %>% filter(Zone != z)
  test <- mydata %>% filter(Zone == z)
  ## train a model for each species
  lm.HP <- gam(HP ~ s(phyto, k=4) , data = train)
  lm.HS <- gam(HS ~ s(bathy,k=3) + s(phyto,k=4) + s(topog,k=4), family = poisson, offset = log(effort), data = train)
  lm.CM <- gam(CM ~ s(dist,k=3) + s(phyto,k=3) + s(temp_sd,k=4) + s(temp,k=4) + s(salt,k=4), family = poisson, offset = log(effort), data = train)
  lm.GL <- gam(GL ~ s(bathy, k=3) + s(dist,k=3) + s(temp,k=4) + s(phyto,k=4), family = poisson, offset = log(effort), data = train)
  ## apply these models to test data
  test$HP.p <- predict(lm.HP, newdata = test, type = "response", se = FALSE)
  test$HS.p <- predict(lm.HS, newdata = test, type = "response", se = FALSE)
  test$CM.p <- predict(lm.CM, newdata = test, type = "response", se = FALSE)
  test$GL.p <- predict(lm.GL, newdata = test, type = "response", se = FALSE)
  ## compute and store validation metrics
  # prediction error and adj. r-squared
  test$HS[test$HS == 0] <- 0.001 # prevent infinite prediction error
  test$HP[test$HP == 0] <- 0.001 # prevent infinite prediction error
  test$CM[test$CM == 0] <- 0.001 # prevent infinite prediction error
  test$GL[test$GL == 0] <- 0.001 # prevent infinite prediction error
  test <- test %>% mutate(
    d.HP = (((HP - HP.p) / HP) * 100),
    d.HS = (((HS - HS.p) / HS) * 100),
    d.CM = (((CM - CM.p) / CM) * 100),
    d.GL = (((GL - GL.p) / GL) * 100)
  )
  data <- data.frame(
    Zone = z,
    Year = rep(c(2017,2018,2019,2020,2021), times = 4),
    species = rep(c("HP", "HS", "CM", "GL"), each = 5),
    R.squared = rep(c(summary(lm.HP)$r.sq, summary(lm.HS)$r.sq, summary(lm.CM)$r.sq, summary(lm.GL)$r.sq), each = 5),
    Dev.Expl = rep(c(summary(lm.HP)$dev.ex, summary(lm.HS)$dev.ex, summary(lm.CM)$dev.ex, summary(lm.GL)$dev.ex), each = 5)
  )
  LZO_raw <- rbind(LZO_raw, test[,c(1,2,13:20)])
  LZO_results <- rbind(LZO_results, data)
  print(z)
  rm(data, train, test, lm.HP, lm.HS, lm.CM, lm.GL)
}

```

# SUPPLEMENTARY ANALYSIS OF VARIANCE
I can perform a similar analysis to examine how much density varies between years in the dataset 
```{r Year ANOVA}
GL$dup <- GL$Date
GL <- GL %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
GL$Year <- factor(GL$Year)
GL <- GL %>% filter(Year == 2017 | Year == 2018 | Year == 2019 | Year == 2020 | Year == 2021)
# visualize the data
ggplot(data = GL, aes(x = Year, y = Density, color = Year)) +
  geom_jitter(alpha = 0.5) +
  geom_boxplot(color = "black", fill = NA) +
  theme_classic()

# run an anova on the data
print(kruskal.test(Density ~ Year, data = GL)) # there are highly significant differences between the zones
# post-hoc testing
pairwise.wilcox.test(GL$Density, GL$Year,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

## CoMu
CM$dup <- CM$Date
CM <- CM %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
CM$Year <- factor(CM$Year)
CM <- CM %>% filter(Year == 2017 | Year == 2018 | Year == 2019 | Year == 2020 | Year == 2021)
# visualize the data
ggplot(data = CM, aes(x = Year, y = Density, color = Year)) +
  geom_jitter(alpha = 0.5) +
  geom_boxplot(color = "black", fill = NA) +
  theme_classic()

# run an anova on the data
print(kruskal.test(Density ~ Year, data = CM)) # there are highly significant differences between the zones
# post-hoc testing
pairwise.wilcox.test(CM$Density, CM$Year,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

```

Now lets consider the variation across cruises within individual years
```{r Seasonal ANOVA}
GL.kw$Date <- as.Date(GL.kw$Date)
GL.kw$Date <- lubridate::ymd(GL.kw$Date)
GL.kw$week <- strftime(GL.kw$Date, format = "%V")
# visualize the data
ggplot(data = GL, aes(x = week, y = Density, color = factor(week), group = week, shape = Year)) +
  geom_jitter(alpha = 0.5) +
  geom_boxplot(color = "black", fill = NA) +
  theme_classic()

# run an anova on the data
print(kruskal.test(Density ~ week, data = GL.kw))

# CoMu
CM$week <- strftime(CM$Date, format = "%V")
# visualize the data
ggplot(data = CM, aes(x = week, y = Density, color = factor(week), group = week, shape = Year)) +
  geom_jitter(alpha = 0.5) +
  geom_boxplot(color = "black", fill = NA) +
  theme_classic()

# run an anova on the data
CM$week <- factor(CM$week)
print(kruskal.test(Density ~ week, data = CM)) # there are highly significant differences between the zones
# post-hoc testing
pairwise.wilcox.test(CM$Density, CM$week,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

# HSeal
HSeal$Date <- as.Date(HSeal$Date)
HSeal$Date <- lubridate::ymd(HSeal$Date)
HSeal$week <- strftime(CM$Date, format = "%V")

# visualize
ggplot(HSeal) +
  geom_boxplot(aes(x=week, y=Count, color = week))

# run an anova on the data
HSeal$week <- factor(HSeal$week)
print(kruskal.test(Density ~ week, data = HSeal)) # there are highly significant differences between the zones
# post-hoc testing
pairwise.wilcox.test(HSeal$Density, HSeal$week,
  p.adjust.method = "bonferroni" # apply Bonferroni's correction
)

# first week of october appears to be nearly significantly different from the other weeks of the season
```


## FORMAT TRAINING DATA
# Compile environmental data for cruise dates
This code will compile the static and dynamic variables on each day that MBM transects were conducted from 2017-2020 and match them with the observed species densities from those days: 
```{r Complile Cruise Data (Training)}
trans_all <- inner_join(env_all, transect_zones) # apply transect zones
trans_all <- trans_all %>% filter(!is.na(transect)) # remove grid cells off transect
trans_all$Date <- as.character(trans_all$Date)

trans_all <- trans_all %>% group_by(Date, transect) %>% summarise(
  bathy = mean(bathy),
  topog = mean(topog),
  dist = mean(dist),
  channel_width = mean(channel_width),
  tcur = mean(tcur)
)

trans_all <- trans_all %>% rename(
  Zone = transect
)

trans_all$Zone <- factor(trans_all$Zone)

## add LiveOcean Data ##
trans_all <- left_join(trans_all, LO_phyto)
trans_all <- left_join(trans_all, LO_temp)
trans_all <- left_join(trans_all, LO_salt)

trans_all$Date <- lubridate::ymd(trans_all$Date)

trans_1 <- inner_join(trans_all, MBM_master, by = c("Date", "Zone"))
trans_1$dup <- trans_1$Date
trans_1 <- trans_1 %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
trans_1 <- trans_1 %>% dplyr::select(-c(Month, Day))
trans_1$Year <- factor(trans_1$Year)
```

# Add presence vs. absence data for marine mammals for logistic regression
This code separates out the zones were MBM were present and assigns them to factor "1". Likewise, zones with none or a particular species are assigned to "0":

```{r Presence vs. Absence}
Present <- trans_1 %>% filter(Density > 0)
Present$PA <- c(rep(1, times = nrow(Present)))

Absent <- trans_1 %>% filter(Density == 0)
Absent$PA <- c(rep(0, times = nrow(Absent)))

trans_1 <- rbind(Present, Absent)
trans_1$PA <- as.factor(trans_1$PA)
rm(Present, Absent)
```

# delta tide height
``` {r dth}
trans_1 <- left_join(trans_1, delta.tide.height[,c(1,4)], by = c("Date"))
```

# scale training data 
```{r scale training}
scaled_vars <- scale(trans_1[,c(3:11,18)])
trans_1 <- trans_1 %>% dplyr::select(c("Date", "Year", "Zone", "species", "Density", "Effort_sqkm", "PA", "count"))
trans_1 <- cbind(trans_1, scaled_vars)
trans_1 <- trans_1 %>% rename(effort = Effort_sqkm)
rm(scaled_vars)
```

# cruise number
```{r cruise number}
trans_1 <- left_join(trans_1, cruises_ocean_time_all[,c(1,4)])
trans_1$cruise.gen <- factor(trans_1$cruise.gen)
```


## FINE-SCALE HABITAT MODELS
This chunk performs correlation analysis on the environmental variables used for construction of fine-scale models:
```{r fine-scale cor.test}
mydata2 <- trans_1[,c(14:18)]

cor.test2 <- cor(mydata2, use = "complete.obs")

chart.Correlation(mydata2)
```

# LOGISTIC REGRESSION MODELS
# Harbor Seals
This chunk performs logistic regression model selection for harbor seals:
```{r Logit: Harbor Seal}
HSeal <- filter(trans_1, species == "HSeal")

HS01 <- glm(PA ~ phyto, data= HSeal, family = "binomial")
HS02 <- glm(PA ~ temp, data= HSeal, family = "binomial")
HS03 <- glm(PA ~ temp_sd, data= HSeal, family = "binomial")
HS04 <- glm(PA ~ salt, data= HSeal, family = "binomial")
HS05 <- glm(PA ~ dth, data= HSeal, family = "binomial")
HS06 <- glmer(PA ~ (1 | Zone), data= HSeal, family = "binomial")
HS07 <- glmer(PA ~ (1 | Year), data= HSeal, family = "binomial")
HS08 <- glmer(PA ~ (1 | cruise.gen), data= HSeal, family = "binomial")

# selecting the best model
vec_AIC <- AIC(HS01, HS02, HS03, HS04, HS05, HS06, HS07)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw
# Examine model results
summary(HS07)
rm(HS01, HS02, HS03, HS04, HS05, HS06, HS08)
```
Year was the best single predictor of harbor seal habitat use

# HSeal: Forward Selection 2
Selecting the second most influential variable for Harbor Seals:

```{r Logit HSeal: Forward Selection 2}
HS5.1 <- glmer(PA ~ phyto + (1 | Year), data= HSeal, family = "binomial")
HS5.2 <- glmer(PA ~ temp + (1 | Year), data= HSeal, family = "binomial")
HS5.3 <- glmer(PA ~ temp_sd + (1 | Year), data= HSeal, family = "binomial")
HS5.4 <- glmer(PA ~ salt + (1 | Year), data= HSeal, family = "binomial")
HS5.5 <- glmer(PA ~ dth + (1 | Year), data= HSeal, family = "binomial")
HS5.6 <- glmer(PA ~ (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS5.7 <- glmer(PA ~ (1 | Year) + (1 | cruise.gen), data= HSeal, family = "binomial")

# calculating AIC weights
vec_AIC <- AIC(HS07, HS5.1, HS5.2, HS5.3, HS5.4, HS5.5, HS5.6, HS5.7)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(HS5.6)
rm(HS5.1, HS5.2, HS5.3, HS5.4, HS5.5, HS5.7)
```
Zone is the next best predictor of harbor seal presence

# HSeal: Forward Selection 3
This chunk explores the thrid best predictor for harbor seals
```{r Logit HSeal: Forward Selection 3}
HS5.6.1 <- glmer(PA ~ phyto + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS5.6.2 <- glmer(PA ~ dth + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS5.6.3 <- glmer(PA ~ temp + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS5.6.4 <- glmer(PA ~ temp_sd + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS5.6.5 <- glmer(PA ~ salt + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS5.6.6 <- glmer(PA ~ (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")

# calculating AIC weights
vec_AIC <- AIC(HS5.6, HS5.6.1, HS5.6.2, HS5.6.3, HS5.6.4, HS5.6.5, HS5.6.6)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(HS5.6.2)
rm(HS5.6.1, HS5.6.3, HS5.6.4, HS5.6.5, HS5.6.6)
```
delta tide height is the next best predictor 

# HSeal: Forward Selection 4
```{r Logit HSeal: Forward Selection 4}
HS6.5.6.1 <- glmer(PA ~ dth + phyto + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS6.5.6.2 <- glmer(PA ~ dth + temp + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS6.5.6.3 <- glmer(PA ~ dth + temp_sd + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS6.5.6.4 <- glmer(PA ~ dth + salt + (1 | Year) + (1 | Zone), data= HSeal, family = "binomial")
HS6.5.6.5 <- glmer(PA ~ dth + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")

# calculating AIC weights
vec_AIC <- AIC(HS5.6.2, HS6.5.6.1, HS6.5.6.2, HS6.5.6.3, HS6.5.6.4, HS6.5.6.5)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(HS6.5.6.5)
rm(HS6.5.6.1, HS6.5.6.2, HS6.5.6.3, HS6.5.6.4)
```
cruise number was the next best predictor but was not significant.

# HSeal: Forward Selection 5
```{r Logit HSeal: Forward Selection 5}
HS6.5.6.5.1 <- glmer(PA ~ dth + phyto + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")
HS6.5.6.5.2 <- glmer(PA ~ dth + temp + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")
HS6.5.6.5.3 <- glmer(PA ~ dth + temp_sd + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")
HS6.5.6.5.4 <- glmer(PA ~ dth + salt + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")

# calculating AIC weights
vec_AIC <- AIC(HS6.5.6.5, HS6.5.6.5.1, HS6.5.6.5.2, HS6.5.6.5.3, HS6.5.6.5.4)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(HS6.5.6.5.1)
rm(HS6.5.6.1, HS6.5.6.2, HS6.5.6.3, HS6.5.6.4)
```
phytoplankton was the next best predictor but not significant

| summary(HS6.5.6.5) |

# Model Fit
```{r Logit HSeal: Assess Model Fit}
# Pseudo R^2 for HSeal Model
nullmod <- glm(PA ~ 1, data = HSeal, family="binomial")
1-logLik(HS6.5.6.5)/logLik(nullmod) # 'log Lik.' 0.2307126 (df=5)

## odds ratios ##
exp(cbind(OR = coef(HS6.5.6.5)))

## analyzing predicted probabilities of each variable ##
newdata1 <- with(HSeal, data.frame(dth = rep(seq(-3, 2, by = 0.05), times = 36), Zone = rep(c(1,2,3,4,5,6), each = 606), Year = rep(c(2017), times= 3636), cruise.gen = rep(rep(c(1,2,3,4,5,6), each= 101),times=6))) # choose variable to model
newdata2 <- with(HSeal, data.frame(dth = rep(seq(-3, 2, by = 0.05), times = 36), Zone = rep(c(1,2,3,4,5,6), each = 606), Year = rep(c(2020), times= 3636), cruise.gen = rep(rep(c(1,2,3,4,5,6), each= 101),times=6))) # choose variable to model
newdata1 <- rbind(newdata1, newdata2)
rm(newdata2)
newdata1$predicted <- predict(HS6.5.6.5, newdata = newdata1, type = "response")
newdata1$Zone <- factor(newdata1$Zone)
newdata1$Year <- factor(newdata1$Year)

# mean effect across cruise gen
newdata2 <- newdata1 %>% group_by(dth, Zone, Year) %>% summarize(pred = mean(predicted))
s.x <- scale(trans_a$dth)
attributes(s.x)
newdata2$dth <- newdata2$dth * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(newdata2) + 
  geom_line(aes(x = dth, y = pred, color = Zone)) +
  facet_wrap(~Year) +
  xlab("∆ Tide Height (m)") +
  ylab("Predicted Probability of Presence") +
  theme_classic() +
  theme(legend.position = "bottom")

trans_1 %>% group_by(Year) %>% summarize(phyto = mean(phyto), p.sd = sd(phyto, na.rm = TRUE), temp = mean(temp), t.sd = sd(temp), temp_sd = mean(temp_sd),tsd.sd = sd(temp_sd), salt = mean(salt), s.sd = sd(salt), dth = mean(dth), dth.sd = sd(dth))

# effect of cruise.gen
lattice::dotplot(ranef(HS6.5.6.5, which = "cruise.gen", condVar = TRUE))
```
testing interspecies interactions:
```{r HSeal: interspec}
HSeal <- left_join(HSeal, species_stmp, by = c("Date", "Zone"))

HS01 <- glmer(PA ~ dth + HP.d + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")
HS02 <- glmer(PA ~ dth + GL.d + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")
HS03 <- glmer(PA ~ dth + CM.d + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal, family = "binomial")

summary(HS01)
summary(HS02)
summary(HS03)

# how much do they improve the model?
vec_AIC <- AIC(HS6.5.6.5, HS01, HS02, HS03)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

rm(HS01, HS02, HS03)
```

------------
| HS6.5.6.5 |
------------

# Harbor Porpoise
This chunk performs logistic regression model selection for harbor seals:
```{r Logit HPorp}
HPorp <- filter(trans_1, species == "HPorp")

HP01 <- glm(PA ~ phyto, data= HPorp, family = "binomial")
HP02 <- glm(PA ~ temp, data= HPorp, family = "binomial")
HP03 <- glm(PA ~ temp_sd, data= HPorp, family = "binomial")
HP04 <- glm(PA ~ salt, data= HPorp, family = "binomial")
HP05 <- glm(PA ~ dth, data= HPorp, family = "binomial")
HP06 <- glmer(PA ~ (1 | Zone), data= HPorp, family = "binomial")
HP07 <- glmer(PA ~ (1 | Year), data= HPorp, family = "binomial")
HP08 <- glmer(PA ~ (1 | cruise.gen), data= HPorp, family = "binomial")
# test
HP0.1 <- gam(PA ~ phyto + s(Year, bs="re"), data= HPorp, family = "binomial")
HP0.2 <- gam(PA ~ temp + s(Year, bs="re"), data= HPorp, family = "binomial")
HP0.3 <- gam(PA ~ temp_sd + s(Year, bs="re"), data= HPorp, family = "binomial")
HP0.4 <- gam(PA ~ salt + s(Year, bs="re"), data= HPorp, family = "binomial")
HP0.5 <- gam(PA ~ dth+ s(Year, bs="re"), data= HPorp, family = "binomial")
HP0.6 <- gam(PA ~ s(Zone, bs = "re"), data= HPorp, family = "binomial")
HP0.7 <- gam(PA ~ s(Year, bs="re"), data= HPorp, family = "binomial")
HP0.8 <- gam(PA ~ s(cruise.gen, bs="re"), data= HPorp, family = "binomial")

# selecting the best model
vec_AIC <- AIC(HP01, HP02, HP03, HP04, HP05, HP06, HP07, HP08)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# Examine model results
summary(HP03)
rm(HP01, HP02, HP03, HP04, HP06, HP07, HP08)
```
Temp_sd was best but not significant

testing interspecies interactions:
```{r HPorp: interspec}
HPorp <- left_join(HPorp, species_stmp, by = c("Date", "Zone"))

HP01 <- glm(PA ~ temp_sd + HS.d, data= HPorp, family = "binomial")
HP02 <- glm(PA ~ temp_sd + GL.d , data= HPorp, family = "binomial")
HP04 <- glm(PA ~ temp_sd + CM.d, data= HPorp, family = "binomial")

summary(HP01)
summary(HP02)
summary(HP04)

# how much do they improve the model?
vec_AIC <- AIC(HP03, HP01, HP02, HP04)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

rm(HS01, HS02, HS03)
```

--------
| HP03 |  ???
--------

# Assessing model fit
```{r Logit HPorp: Assess Model Fit}
# Pseudo R^2 for HSeal Model
nullmod <- glm(PA ~ 1, data = HPorp, family="binomial")
1-logLik(HP03)/logLik(nullmod)

# confidence intervals for model coefficients
confint.default(HP03)

## odds ratios ##
exp(cbind(OR = coef(HP03)))

## analyzing predicted probabilities of each variable ##
newdata1 <- with(HPorp, data.frame(temp_sd = seq(-2, 4, 0.05)))
newdata2 <- cbind(newdata1, predict(HP03, newdata = newdata1, type = "link", se = TRUE))
newdata2 <- within(newdata2, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})

ggplot(newdata2, aes(x = temp_sd, y = PredictedProb)) + 
  geom_ribbon(aes(ymin = LL, ymax = UL), linetype = "dashed", alpha = 0.2) + 
  geom_line() +
  xlab("SST SD (ºC)") +
  ylab("Predicted Probability of Presence") +
  theme_classic()

```

# Predictive surfaces for marine mammals
This chunk will make daily predictions of marine mammal densities across the study area and then average them into a seasonal estimate of habitat use:

```{r Predicted Habitat Map}
## Predicting Across Study Grid Using Environmental Variables ##
## HARBOR SEAL ##
HS_Predicted_Presence <- env_all
scaled.vars <- scale(HS_Predicted_Presence[,c(6:10,12:15)])
HS_Predicted_Presence <- HS_Predicted_Presence %>% dplyr::select(c("grid_id", "lat_deg", "long_deg", "Date"))
HS_Predicted_Presence <- cbind(HS_Predicted_Presence, scaled.vars)
rm(scaled.vars)


HS_Predicted_Presence$predicted <- predict(HS1.1.3, newdata = HS_Predicted_Presence, type = "response")

HS_Predicted_Presence <- HS_Predicted_Presence %>% group_by(grid_id) %>% dplyr::summarise(lat_deg = mean(lat_deg), long_deg = mean(long_deg), sd = sd(predicted), predicted = mean(predicted))

HS_Predicted_Presence <- HS_Predicted_Presence %>% mutate(CV = sd/predicted)

HS_Predicted_Presence <- HS_Predicted_Presence %>% st_as_sf(coords = c("long_deg", "lat_deg"), crs = 4326)

ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = HS_Predicted_Presence, aes(fill = predicted), shape = 22, size = 2.5) +
  scale_fill_viridis(option = "magma", name = "Probability") +
  ggtitle("Harbor Seal Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal() +
  theme(legend.position = "bottom")

## HARBOR PORPOISE ##
HP_Predicted_Presence <- env_all
scaled.vars <- scale(HP_Predicted_Presence[,c(6:10,12:15)])
HP_Predicted_Presence <- HP_Predicted_Presence %>% dplyr::select(c("grid_id", "lat_deg", "long_deg", "Date"))
HP_Predicted_Presence <- cbind(HP_Predicted_Presence, scaled.vars)
rm(scaled.vars)

HP_Predicted_Presence$predicted <- predict(HP03, newdata = HP_Predicted_Presence, type = "response")

HP_Predicted_Presence <- HP_Predicted_Presence %>% group_by(grid_id) %>% summarise(lat_deg = mean(lat_deg), long_deg = mean(long_deg), sd = sd(predicted), predicted = mean(predicted))

HP_Predicted_Presence <- HP_Predicted_Presence %>% mutate(CV = sd/predicted)

HP_Predicted_Presence <- HP_Predicted_Presence %>% filter(!is.na(predicted))

HP_Predicted_Presence <- HP_Predicted_Presence %>% st_as_sf(coords = c("long_deg", "lat_deg"), crs = 4326)

ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = HP_Predicted_Presence, aes(fill = predicted), shape = 22, size = 2.5) +
  scale_fill_viridis(option = "magma", name = "Probability") +
  ggtitle("Harbor Porpoise Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal() +
  theme(legend.position = "bottom")

# COMBINED #
HS_Predicted_Presence$species <- "HS"
HP_Predicted_Presence$species <- "HP"

MM <- rbind(HS_Predicted_Presence, HP_Predicted_Presence)

ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = MM, aes(fill = predicted), shape = 22, size = 2.5) +
  scale_fill_viridis(option = "magma", name = "") +
  scale_x_continuous(breaks = c(-123.2, -123, -122.8)) +
  facet_wrap(~species) +
  coord_sf() +
  theme_minimal() +
  theme(legend.position = "top")

```

## GAM 2.0 - OFFSET POISSON DISTRIBUTION
Using density data for model training violates a key assumption of the Poisson distribution, since density does not have to be an integer value. In order to better meet the assumptions of the Poisson distribution, we will instead use raw species counts in each zone as the training data and add an "offset term" for zonal area or survey effort. This requires some restructuring of the training data. 

## MODEL SELECTION 2.O
# Glaucous Gull 
```{r GAM2 GL}
GL <- filter(trans_1, species == "GL")
CoMu <- filter(trans_1, species == "CoMu")
CM_count <- CoMu %>% dplyr::select(c("count", "Density"))
CM_count <- CM_count %>% rename(CM.c = count, CM.d = Density)
GL <- cbind(GL, CM_count)

GL01 <- gam(count ~ offset(log(effort)) + s(phyto, k=4), family = poisson(link = "log"), data=GL)
GL02 <- gam(count ~ offset(log(effort)) + s(temp, k=4), family = poisson(link = "log"), data=GL)
GL03 <- gam(count ~ offset(log(effort)) + s(temp_sd, k=4), family = poisson(link = "log"), data=GL)
GL04 <- gam(count ~ offset(log(effort)) + s(salt, k=4), family = poisson(link = "log"), data=GL)
GL05 <- gam(count ~ offset(log(effort)) + s(dth, k=4), family = poisson(link = "log"), data=GL)
GL06 <- gam(count ~ offset(log(effort)) + s(Zone, bs="re"), family = poisson(link = "log"), data=GL)
GL07 <- gam(count ~ offset(log(effort)) + s(Year, bs="re"), family = poisson(link = "log"), data=GL)
GL08 <- gam(count ~ offset(log(effort)) + s(cruise.gen, bs="re"), family = poisson(link = "log"), data=GL)


# selecting the best model
vec_AIC <- AIC(GL01, GL02, GL03, GL04, GL05, GL06, GL07, GL08)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(GL06)
rm(GL01, GL02, GL03, GL04, GL05, GL07, GL08)
```

# GL: Forward Selection 2.2
```{r GAM2 GL:FS2}
GL1.01 <- gam(count ~ offset(log(effort)) + s(phyto,k=4) + s(Zone, bs="re"), family = poisson(link = "log"), data=GL)
GL1.02 <- gam(count ~ offset(log(effort)) + s(temp,k=4) + s(Zone, bs="re"), family = poisson(link = "log"), data=GL)
GL1.03 <- gam(count ~ offset(log(effort)) + s(temp_sd,k=4) + s(Zone, bs="re"), family = poisson(link = "log"), data=GL)
GL1.04 <- gam(count ~ offset(log(effort)) + s(salt,k=4) + s(Zone, bs="re"), family = poisson(link = "log"), data=GL)
GL1.05 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(Zone, bs="re"), family = poisson(link = "log"), data=GL)
GL1.06 <- gam(count ~ offset(log(effort)) + s(Zone, bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=GL)
GL1.07 <- gam(count ~ offset(log(effort)) + s(Zone, bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)

# selecting the best model
vec_AIC <- AIC(GL06, GL1.01, GL1.02, GL1.03, GL1.04, GL1.05, GL1.06, GL1.07)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(GL1.06)
plot(GL1.03)

rm(GL1.01, GL1.02, GL1.03, GL1.04, GL1.05, GL1.07)
```

# GL: Forward Selection 2.3
```{r GAM2 GL:FS3}
GL1.1.01 <- gam(count ~ offset(log(effort)) + s(phyto,k=4) + s(Zone, bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.02 <- gam(count ~ offset(log(effort)) + s(temp,k=4) + s(Zone, bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.03 <- gam(count ~ offset(log(effort)) + s(temp_sd,k=4) + s(Zone, bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.04 <- gam(count ~ offset(log(effort)) + s(salt,k=4) + s(Zone, bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.05 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(Zone, bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.06 <- gam(count ~ offset(log(effort)) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)

# selecting the best model
vec_AIC <- AIC(GL1.06, GL1.1.01, GL1.1.02, GL1.1.03, GL1.1.04, GL1.1.05, GL1.1.06)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(GL1.1.06)

rm(GL1.1.01, GL1.1.02, GL1.1.03, GL1.1.04, GL1.1.05)
```

# GL: Forward Selection 2.4
```{r GAM2 GL:FS4}
GL1.1.1.01 <- gam(count ~ offset(log(effort)) + s(phyto,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.1.02 <- gam(count ~ offset(log(effort)) + s(temp,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.1.03 <- gam(count ~ offset(log(effort)) + s(temp_sd,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.1.04 <- gam(count ~ offset(log(effort)) + s(salt,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.1.05 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)

# selecting the best model
vec_AIC <- AIC(GL1.1.06, GL1.1.1.01, GL1.1.1.02, GL1.1.1.03, GL1.1.1.04, GL1.1.1.05)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(GL1.1.1.05)
rm(GL1.1.1.01, GL1.1.1.02, GL1.1.1.03, GL1.1.1.04)
```

# GL: Forward Selection 2.5
```{r GAM2 GL:FS5}
GL1.1.1.1.01 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(phyto,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.1.1.02 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(temp,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.1.1.03 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(temp_sd,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)
GL1.1.1.1.04 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(salt,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)

# selecting the best model
vec_AIC <- AIC(GL1.1.1.05, GL1.1.1.1.01, GL1.1.1.1.02, GL1.1.1.1.03, GL1.1.1.1.04)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(GL1.1.1.1.02)
rm(GL1.1.1.1.01, GL1.1.1.1.03, GL1.1.1.1.04)
```

# GL: Forward Selection 2.6
```{r GAM2 GL:FS6}
GL1.1.1.1.1.01 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(temp,k=4) + s(salt,k=4) + s(Zone, bs="re") + s(Year,bs="re") + s(cruise.gen,bs="re"), family = poisson(link = "log"), data=GL)

# selecting the best model
vec_AIC <- AIC(GL1.1.1.1.02, GL1.1.1.1.1.01)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(GL1.1.1.1.1.01)
```

-----------------
| GL1.1.1.1.1.01 |
-----------------

Calculate overdispersion parameter:
```{r overdispersion}
plot(residuals(GL1.1.1.1.1.01, type = "deviance")) # values should be close to zero - larger distance signifies outliers
sum((residuals(GL1.1.1.1.1.01, type = "deviance"))^2) # residual deviance = sum of squared deviance residuals
GL1.1.1.1.1.01
```
This value is much larger than 1 - suggests that a negative binomial distribution should be used.

# Common Murre 
```{r GAM2 CoMu}
CoMu <- filter(trans_1, species == "CoMu")
GL_count <- GL %>% dplyr::select(c("count", "Density"))
GL_count <- GL_count %>% rename(GL.c = count, GL.d = "Density")
CoMu <- cbind(CoMu, GL_count)

CM01 <- gam(count ~ offset(log(effort)) + s(phyto, k=4), family = poisson(link = "log"), data=CoMu)
CM02 <- gam(count ~ offset(log(effort)) + s(temp, k=4), family = poisson(link = "log"), data=CoMu)
CM03 <- gam(count ~ offset(log(effort)) + s(temp_sd, k=4), family = poisson(link = "log"), data=CoMu)
CM04 <- gam(count ~ offset(log(effort)) + s(salt, k=4), family = poisson(link = "log"), data=CoMu)
CM05 <- gam(count ~ offset(log(effort)) + s(dth, k=4), family = poisson(link = "log"), data=CoMu)
CM06 <- gam(count ~ offset(log(effort)) + s(Zone, bs="re"), family = poisson(link = "log"), data=CoMu)
CM07 <- gam(count ~ offset(log(effort)) + s(Year, bs="re"), family = poisson(link = "log"), data=CoMu)
CM08 <- gam(count ~ offset(log(effort)) + s(cruise.gen, bs="re"), family = poisson(link = "log"), data=CoMu)


# selecting the best model
vec_AIC <- AIC(CM01, CM02, CM03, CM04, CM05, CM06, CM07, CM08)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(CM08)
rm(CM01, CM02, CM04, CM05, CM06, CM07, CM03)
```

# CoMu: Forward Selection 2 
```{r GAM2 CoMu:F2}
CM1.01 <- gam(count ~ offset(log(effort)) + s(phyto,k=4) + s(cruise.gen, bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.02 <- gam(count ~ offset(log(effort)) + s(temp,k=4) + s(cruise.gen, bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.03 <- gam(count ~ offset(log(effort)) + s(temp_sd,k=4) + s(cruise.gen, bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.04 <- gam(count ~ offset(log(effort)) + s(salt,k=4) + s(cruise.gen, bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.05 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(cruise.gen, bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.06 <- gam(count ~ offset(log(effort)) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.07 <- gam(count ~ offset(log(effort)) + s(cruise.gen, bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=CoMu)

# selecting the best model
vec_AIC <- AIC(CM08, CM1.01, CM1.02, CM1.03, CM1.04, CM1.05, CM1.06, CM1.07)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(CM1.06)
rm(CM1.01, CM1.02, CM1.03, CM1.04, CM1.05, CM1.07)
```

# CoMu: Forward Selection 3 
```{r GAM2 CoMu:F3}
CM1.1.01 <- gam(count ~ offset(log(effort)) + s(phyto,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.02 <- gam(count ~ offset(log(effort)) + s(temp,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.03 <- gam(count ~ offset(log(effort)) + s(temp_sd,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.04 <- gam(count ~ offset(log(effort)) + s(salt,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.05 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.06 <- gam(count ~ offset(log(effort)) + s(cruise.gen, bs="re") + s(Zone,bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=CoMu)


# selecting the best model
vec_AIC <- AIC(CM1.06, CM1.1.01, CM1.1.02, CM1.1.03, CM1.1.04, CM1.1.05, CM1.1.06)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(CM1.1.05)
rm(CM1.1.01, CM1.1.02, CM1.1.03, CM1.1.04, CM1.1.06)
```

# CoMu: Forward Selection 4 
```{r GAM2 CoMu:F4}
CM1.1.1.01 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(phyto,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.1.02 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(temp,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.1.03 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(temp_sd,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.1.04 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(salt,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.1.05 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=CoMu)

# selecting the best model
vec_AIC <- AIC(CM1.1.05, CM1.1.1.01, CM1.1.1.02, CM1.1.1.03, CM1.1.1.04, CM1.1.1.05)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(CM1.1.1.04)
rm(CM1.1.1.01, CM1.1.1.02, CM1.1.1.03, CM1.1.1.05)
```

# CoMu: Forward Selection 5 
```{r GAM2 CoMu:F5}
CM1.1.1.1.01 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(salt,k=4) + s(temp,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.1.1.02 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(salt,k=4) + s(temp_sd,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re"), family = poisson(link = "log"), data=CoMu)
CM1.1.1.1.03 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(salt,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=CoMu)

# selecting the best model
vec_AIC <- AIC(CM1.1.1.04, CM1.1.1.1.01, CM1.1.1.1.02, CM1.1.1.1.03)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(CM1.1.1.1.01)
rm(CM1.1.1.1.02, CM1.1.1.1.03)
```

# CoMu: Forward Selection 6 
```{r GAM2 CoMu:F6}
CM1.1.1.1.1.01 <- gam(count ~ offset(log(effort)) + s(dth,k=4) + s(salt,k=4) + s(temp,k=4) + s(cruise.gen, bs="re") + s(Zone,bs="re") + s(Year,bs="re"), family = poisson(link = "log"), data=CoMu)

# selecting the best model
vec_AIC <- AIC(CM1.1.1.1.01, CM1.1.1.1.1.01)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(CM1.1.1.1.1.01)
```

-----------------
| CM1.1.1.1.1.01 |
-----------------

Calculate overdispersion parameter:
```{r CM overdispersion}
sum((residuals(CM1.1.1.1.1.01, type = "deviance"))^2)
CM1.1.1.1.1.01
```
This value is much larger than 1 - suggests that a negative binomial distribution should be used.

## GENERALIZED ADDITIVE MODELS: NEGATIVE BINOMIAL
# Glaucous Winged Gull
This chunk performs generalized additive model selection for glaucous winged gulls:
```{r GAM GL}
nb.GL01 <- gam(Density ~ s(phyto, k=4), family = nb, data=GL)
nb.GL02 <- gam(Density ~ s(temp, k=4), family = nb, data=GL)
nb.GL03 <- gam(Density ~ s(temp_sd, k=4), family = nb, data=GL)
nb.GL04 <- gam(Density ~ s(salt, k=4), family = nb, data=GL)
nb.GL05 <- gam(Density ~ s(dth, k=4), family = nb, data=GL)
nb.GL06 <- gam(Density ~ s(Zone, bs="re"), family = nb, data=GL)
nb.GL07 <- gam(Density ~ s(Year, bs="re"), family = nb, data=GL)
nb.GL08 <- gam(Density ~ s(cruise.gen, bs="re"), family = nb, data=GL)

# calculate AIC weights
vec_AIC <- AIC(nb.GL01, nb.GL02, nb.GL03, nb.GL04, nb.GL05, nb.GL06, nb.GL07, nb.GL08)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# Examine model outputs
summary(nb.GL06)
rm(nb.GL01, nb.GL02, nb.GL03, nb.GL04, nb.GL05, nb.GL07, nb.GL08)
```
Zone is the best single predictor

# GL: Forward Selection 2
```{r GAM GL: Forward Selection 2}
nb.GL1.1 <- gam(Density ~ s(phyto,k=4) + s(Zone, bs="re"), family = nb, data=GL)
nb.GL1.2 <- gam(Density ~ s(temp,k=4) + s(Zone, bs="re"), family = nb, data=GL)
nb.GL1.3 <- gam(Density ~ s(temp_sd,k=4) + s(Zone, bs="re"), family = nb, data=GL)
nb.GL1.4 <- gam(Density ~ s(salt,k=4) + s(Zone, bs="re"), family = nb, data=GL)
nb.GL1.5 <- gam(Density ~ s(dth,k=4) + s(Zone, bs="re"), family = nb, data=GL)
nb.GL1.6 <- gam(Density ~ s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL1.7 <- gam(Density ~ s(Zone, bs="re") + s(cruise.gen,bs="re"), family = nb, data=GL)

# calculate AIC weights
vec_AIC <- AIC(nb.GL06, nb.GL1.1, nb.GL1.2, nb.GL1.3, nb.GL1.4, nb.GL1.5, nb.GL1.6, nb.GL1.7)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examine model outputs
summary(nb.GL1.6)
rm(nb.GL1.1, nb.GL1.2, nb.GL1.3, nb.GL1.4, nb.GL1.5, nb.GL1.7)

```
Year is the next best predictor

# GL: Forward Selection 3
```{r GAM GL: Forward Selection 3}
nb.GL1.1.1 <- gam(Density ~ s(phyto,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL1.1.2 <- gam(Density ~ s(temp,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL1.1.3 <- gam(Density ~ s(temp_sd,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL1.1.4 <- gam(Density ~ s(salt,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL1.1.5 <- gam(Density ~ s(dth,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL1.1.6 <- gam(Density ~ s(Zone, bs="re") + s(Year,bs = "re") + s(cruise.gen, bs="re"), family = nb, data=GL)

# calculate AIC weights
vec_AIC <- AIC(nb.GL1.6, nb.GL1.1.1, nb.GL1.1.3, nb.GL1.1.4, nb.GL1.1.5, nb.GL1.1.6)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examine model outputs
summary(nb.GL1.1.2)

rm(nb.GL1.1.1, nb.GL1.1.3, nb.GL1.1.4, nb.GL1.1.5, nb.GL1.1.6)
```
Temperature is the next best predictor

# GL: Forward Selection 4
```{r GAM GL: Forward Selection 4}
nb.GL1.1.1.1 <- gam(Density ~ s(temp,k=4) + s(salt,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL1.1.1.2 <- gam(Density ~ s(temp,k=4) + s(Zone, bs="re") + s(Year,bs = "re") + s(cruise.gen,bs="re") , family = nb, data=GL)

# calculate AIC weights
vec_AIC <- AIC(nb.GL1.1.2, nb.GL1.1.1.1, nb.GL1.1.1.2)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining models
rm(nb.GL1.1.1.1, nb.GL1.1.1.2)
```
neither of these terms substantially improved the model

# FS 5: Examining if interspecies effects are present
```{r GAM GL: Forward Selection 5}
GL <- left_join(GL, species_stmp, by = c("Date", "Zone"))

nb.GL0.1 <- gam(Density ~ s(temp,k=4) + s(CM.d.x,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL0.2 <- gam(Density ~ s(temp,k=4) + s(HS.d,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)
nb.GL0.3 <- gam(Density ~ s(temp,k=4) + s(HP.d,k=4) + s(Zone, bs="re") + s(Year,bs = "re"), family = nb, data=GL)


# calculate AIC weights
vec_AIC <- AIC(nb.GL1.1.2, nb.GL0.1, nb.GL0.2, nb.GL0.3)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(nb.GL0.1)
```
# this variable did improve the model

---------------
| nb.GL1.1.2 |
---------------
OR 
-----------
| nb.GL0.1 |
-----------

# Visualizing Fine-Scale model:
```{r Viz FSM: GL}
## PLOTTING MODEL RESULTS: GL
# SST
newdata1 <- with(GL, data.frame(temp = rep(seq(-2,3,by=0.05), times = 36), Zone = rep(c(1,2,3,4,5,6), each=606), Year = c(2017), cruise.gen = rep(rep(c(1,2,3,4,5,6), each = 101), times = 6)))
newdata2 <- with(GL, data.frame(temp = rep(seq(-2,3,by=0.05), times = 36), Zone = rep(c(1,2,3,4,5,6), each=606), Year = c(2020), cruise.gen = rep(rep(c(1,2,3,4,5,6), each = 101), times = 6)))
newdata2 <- rbind(newdata1, newdata2)
rm(newdata1)
# choose variable to model
newdata2$predicted <- predict(nb.GL1.1.2, newdata = newdata2, type = "response")
newdata2$Zone <- factor(newdata2$Zone)
newdata2$Year <- factor(newdata2$Year)

newdata1 <- newdata2 %>% group_by(temp, Zone, Year) %>% summarise(pred = mean(predicted))
s.x <- scale(trans_a$temp)
attributes(s.x)
newdata1$temp <- newdata1$temp * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')
  
ggplot(data = newdata1) + 
  geom_line(aes(x = temp, y = pred, color = Zone)) +
  facet_wrap(~Year) +
  xlab("SST (ºC)") +
  ylab("Count (#)") +
  theme_classic() +
  theme(legend.position = "bottom") # murres are more common farther from shore

```

# Common Murre
This chunk performs generalized additive model selection for common murre:
```{r GAM CoMu}
nb.CM01 <- gam(Density ~ s(phyto, k=4), family = nb, data=CoMu)
nb.CM02 <- gam(Density ~ s(temp, k=4), family = nb, data=CoMu)
nb.CM03 <- gam(Density ~ s(temp_sd, k=4), family = nb, data=CoMu)
nb.CM04 <- gam(Density ~ s(salt, k=4), family = nb, data=CoMu)
nb.CM05 <- gam(Density ~ s(dth, k=4), family = nb, data=CoMu)
nb.CM06 <- gam(Density ~ s(Zone, bs="re"), family = nb, data=CoMu)
nb.CM07 <- gam(Density ~ s(Year, bs="re"), family = nb, data=CoMu)
nb.CM08 <- gam(Density ~ s(cruise.gen, bs="re"), family = nb, data=CoMu)

# calculating AIC weights
vec_AIC <- AIC(nb.CM01, nb.CM02, nb.CM03, nb.CM04, nb.CM05, nb.CM06, nb.CM07, nb.CM08)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# Examining Model Results
summary(nb.CM06)
rm(nb.CM01, nb.CM02, nb.CM03, nb.CM04, nb.CM05, nb.CM07, nb.CM08)
```
Based on AIC weights, Zone produces the best predictive model.

#CoMu: Forward Selection 2
```{r GAM CoMu: Forward Selection 2}
nb.CM1.1 <- gam(Density ~ s(phyto,k=4) + s(Zone, bs="re"), family = nb, data=CoMu)
nb.CM1.2 <- gam(Density ~ s(temp,k=4) + s(Zone, bs="re"), family = nb, data=CoMu)
nb.CM1.3 <- gam(Density ~ s(temp_sd,k=4) + s(Zone, bs="re"), family = nb, data=CoMu)
nb.CM1.4 <- gam(Density ~ s(salt,k=4) + s(Zone, bs="re"), family = nb, data=CoMu)
nb.CM1.5 <- gam(Density ~ s(dth,k=4) + s(Zone, bs="re"), family = nb, data=CoMu)
nb.CM1.6 <- gam(Density ~ s(Zone, bs="re") + s(Year, bs = "re"), family = nb, data=CoMu)
nb.CM1.7 <- gam(Density ~ s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)

# selecting the best model
vec_AIC <- AIC(nb.CM06, nb.CM1.1, nb.CM1.2, nb.CM1.3, nb.CM1.4, nb.CM1.5, nb.CM1.6, nb.CM1.7)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining best model
summary(nb.CM1.7)

rm(nb.CM1.1, nb.CM1.2, nb.CM1.3, nb.CM1.4, nb.CM1.5, nb.CM1.6) # cleanup

```
Based on these results, cruise week was the best predictor 

#CoMu: Forward Selection 3
```{r GAM CoMu: Forward Selection 3}
nb.CM1.1.1 <- gam(Density ~ s(phyto,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.2 <- gam(Density ~ s(temp,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.3 <- gam(Density ~ s(temp_sd,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.4 <- gam(Density ~ s(salt,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.5 <- gam(Density ~ s(dth,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.6 <- gam(Density ~ s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year,bs="re"), family = nb, data=CoMu)

# selecting the best model
vec_AIC <- AIC(nb.CM1.7, nb.CM1.1.1, nb.CM1.1.2, nb.CM1.1.3, nb.CM1.1.4, nb.CM1.1.5, nb.CM1.1.6)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(nb.CM1.1.5)
rm(nb.CM1.1.1, nb.CM1.1.2, nb.CM1.1.3, nb.CM1.1.4, nb.CM1.1.6)
```
∆ Tide Height is the next best predictor

#CoMu: Forward Selection 4
```{r GAM CoMu: Forward Selection 4}
nb.CM1.1.1.1 <- gam(Density ~ s(dth,k=4) + s(phyto,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.1.2 <- gam(Density ~ s(dth,k=4) + s(temp,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.1.3 <- gam(Density ~ s(dth,k=4) + s(temp_sd,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.1.4 <- gam(Density ~ s(dth,k=4) + s(salt,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re"), family = nb, data=CoMu)
nb.CM1.1.1.5 <- gam(Density ~ s(dth,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs="re"), family = nb, data=CoMu)

# selecting the best model
vec_AIC <- AIC(nb.CM1.1.5, nb.CM1.1.1.1, nb.CM1.1.1.2, nb.CM1.1.1.3, nb.CM1.1.1.4, nb.CM1.1.1.5)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(nb.CM1.1.1.5)

rm(nb.CM1.1.1.2, nb.CM1.1.1.3, nb.CM1.1.1.4, nb.CM1.1.1.1)
```
Year is the next most significant variable

#CoMu: Forward Selection 5
```{r GAM CoMu: Forward Selection 5}
nb.CM1.1.1.1.1 <- gam(Density ~ s(dth,k=4) + s(phyto,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs="re"), family = nb, data=CoMu)
nb.CM1.1.1.1.2 <- gam(Density ~ s(dth,k=4) + s(temp,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs="re"), family = nb, data=CoMu)
nb.CM1.1.1.1.3 <- gam(Density ~ s(dth,k=4) + s(temp_sd,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs="re"), family = nb, data=CoMu)
nb.CM1.1.1.1.4 <- gam(Density ~ s(dth,k=4) + s(salt,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs="re"), family = nb, data=CoMu)

# selecting the best model
vec_AIC <- AIC(nb.CM1.1.1.5, nb.CM1.1.1.1.1, nb.CM1.1.1.1.2, nb.CM1.1.1.1.3, nb.CM1.1.1.1.4)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(nb.CM1.1.1.1.1)
rm(nb.CM1.1.1.1.2, nb.CM1.1.1.1.3, nb.CM1.1.1.1.4)
```
[phytoplankton] is the final predictive variable

# FS 6: Examining if interspecies effects are present
```{r GAM CoMu: Forward Selection 5}
CoMu <- left_join(CoMu, species_stmp, by = c("Date", "Zone"))

nb.CM0.1 <- gam(Density ~ s(dth,k=4) + s(phyto,k=4) + s(GL.d.x,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs = "re"), family = nb, data=CoMu)
nb.CM0.2 <- gam(Density ~ s(dth,k=4) + s(phyto,k=4) + s(HS.d,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs = "re"), family = nb, data=CoMu)
nb.CM0.3 <- gam(Density ~ s(dth,k=4) + s(phyto,k=4) + s(HP.d,k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs = "re"), family = nb, data=CoMu)
nb.CM0.4 <- gam(Density ~ s(dth,k=4) + s(phyto,k=4) + s(GL.d.x,k=4) + s(HS.d, k=4) + s(Zone, bs="re") + s(cruise.gen, bs = "re") + s(Year, bs = "re"), family = nb, data=CoMu)



# selecting the best model
vec_AIC <- AIC(nb.CM1.1.1.1.1, nb.CM0.1, nb.CM0.2, nb.CM0.3, nb.CM0.4)[,2]
vec_AIC
dAIC <- vec_AIC - min(vec_AIC)
AICw <- exp(-dAIC/2) / sum(exp(-dAIC/2))
AICw

# examining model results
summary(nb.CM0.1)
summary(nb.CM0.2)
summary(nb.CM0.3)
summary(nb.CM0.4)
```

-----------------
| nb.CM1.1.1.1.1 |
-----------------
OR
-----------
| nb.CM0.1 |
-----------

# Visualizing Fine-Scale model:
```{r Viz FSM: CoMu}
## PLOTTING MODEL RESULTS: COMU
# PHYTO
newdata1 <- with(CoMu, data.frame(dth = mean(dth), phyto = rep(seq(-2,4,by=0.05), times = 36), Zone = rep(c(1,2,3,4,5,6), each=726), Year = c(2017), cruise.gen = rep(rep(c(1,2,3,4,5,6), each = 121), times = 6)))
newdata2 <- with(CoMu, data.frame(dth = mean(dth), phyto = rep(seq(-2,4,by=0.05), times = 36), Zone = rep(c(1,2,3,4,5,6), each=726), Year = c(2020), cruise.gen = rep(rep(c(1,2,3,4,5,6), each = 121), times = 6)))
newdata2 <- rbind(newdata1, newdata2)
rm(newdata1)
# choose variable to model
newdata2$predicted <- predict(nb.CM1.1.1.1.1, newdata = newdata2, type = "response")
newdata2$Zone <- factor(newdata2$Zone)

newdata1 <- newdata2 %>% group_by(phyto, Zone, Year) %>% summarise(pred = mean(predicted))
s.x <- scale(trans_a$phyto)
newdata1$phyto <- newdata1$phyto * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')

ggplot(data = newdata1) + 
  geom_line(aes(x = phyto, y = pred, color = Zone)) +
  facet_wrap(~Year) +
  xlab("[Phytoplankton] (µmol/L)") +
  ylab("Count (#)") +
  theme_classic() + 
  theme(legend.position = "bottom") # murres are more common farther from shore

# ∆ TIDE HEIGHT
newdata1 <- with(CoMu, data.frame(dth = rep(seq(-3,2,by=0.05), times = 36), phyto = mean(phyto) , Zone = rep(c(1,2,3,4,5,6), each=606), Year = c(2017), cruise.gen = rep(rep(c(1,2,3,4,5,6), each = 101), times = 6)))
newdata2 <- with(CoMu, data.frame(dth = rep(seq(-3,2,by=0.05), times = 36), phyto = mean(phyto), Zone = rep(c(1,2,3,4,5,6), each=606), Year = c(2020), cruise.gen = rep(rep(c(1,2,3,4,5,6), each = 101), times = 6)))
newdata2 <- rbind(newdata1, newdata2)
rm(newdata1)
# choose variable to model
newdata2$predicted <- predict(nb.CM1.1.1.1.1, newdata = newdata2, type = "response")
newdata2$Zone <- factor(newdata2$Zone)
newdata2$Year <- factor(newdata2$Year)

newdata1 <- newdata2 %>% group_by(dth, Zone, Year) %>% summarise(pred = mean(predicted))
s.x <- scale(trans_a$dth)
newdata1$dth <- newdata1$dth * attr(s.x, 'scaled:scale') + attr(s.x, 'scaled:center')


ggplot(data = newdata1) + 
  geom_line(aes(x = dth, y = pred, color = Zone)) +
  facet_wrap(~Year) +
  xlab("∆ Tide Height (m)") +
  ylab("Count (#)") +
  theme_classic() + 
  theme(legend.position = "bottom") # murres are more common farther from shore

# cruise gen
newdata1 <- with(CoMu, data.frame(dth = 0, phyto = 0 , Zone = rep(c(1,2,3,4,5,6), each=6), Year = 2017, cruise.gen = rep(c(1,2,3,4,5,6), times = 6)))

lattice::dotplot(extract_ranef(nb.CM1.1.1.1.1, which = "cruise.gen", condVar = TRUE))
```


## CROSS VALIDATION METHODS:
Test the methodology for Monte Carlo cross validation on a single iteration of holdout validation

# Step 1: Begin by creating a training and test set on which to build and apply validation models
Partition the data into training and test sets:
```{r CrVal: Data Partitioning, echo=FALSE}
allrows <- 1:nrow(cruises_ocean_time_all)

set.seed(7)
train_rows <- sample(allrows, replace = F, size = 0.75*length(allrows))
test_rows <- allrows[-train_rows]

train <- cruises_ocean_time_all[train_rows,]
train <- train %>% dplyr::select(-c(ocean_time))
train$Date <- ymd(train$Date)
test <- cruises_ocean_time_all[test_rows,]
test <- test %>% dplyr::select(-c(ocean_time))
test$Date <- ymd(test$Date)

train <- inner_join(trans_1, train)
test <- inner_join(trans_1, test)

# Note: How many unique combinations of training / testing data are there in this dataset?
(factorial(28) / ((28-21) * factorial(21))) # calculate the number of combinations (not permutations)
# a shit fuck ton is the answer
```

# Step 1.5: Alternatively, begin by creating a stratified training and test with equal representation from each year:
Partition the data into training and test sets:
```{r CrVal: Data Partitioning, echo=FALSE}
rows_2017 <- 1:6
rows_2018 <- 7:12
rows_2019 <- 13:17
rows_2020 <- 18:24
rows_2021 <- 25:28

set.seed(7)
strat.2017 <- sample(rows_2017, replace = F, size = 0.75*length(rows_2017))
strat.2018 <- sample(rows_2018, replace = F, size = 0.75*length(rows_2018))
strat.2019 <- sample(rows_2019, replace = F, size = 0.75*length(rows_2019))
strat.2020 <- sample(rows_2020, replace = F, size = 0.75*length(rows_2020))
strat.2021 <- sample(rows_2021, replace = F, size = 0.75*length(rows_2021))


train_rows <- c(strat.2017, strat.2018, strat.2019, strat.2020, strat.2021)
test_rows <- allrows[-train_rows]

cruises_ocean_time_all$cruise.gen <- factor(cruises_ocean_time_all$cruise.gen)
train <- cruises_ocean_time_all[train_rows,]
train <- train %>% dplyr::select(-c(ocean_time))
train$Date <- ymd(train$Date)
test <- cruises_ocean_time_all[test_rows,]
test <- test %>% dplyr::select(-c(ocean_time))
test$Date <- ymd(test$Date)

train <- inner_join(trans_1, train)
test <- inner_join(trans_1, test)

# Note: How many unique combinations of training / testing data are there in this dataset?
(factorial(28) / ((28-21) * factorial(21))) # calculate the number of combinations (not permutations)
# a shit fuck ton is the answer
```

# Step 2: Add predicted values of MBM habitat occupancy 
This code adds prediction values for each of my study species based on the conditions in each zone throughout the 2021 cruises. 
```{r 2021 Seabird Predictions}
GL_train <- train %>% filter(species == "GL")
GL_test <- test %>% filter(species == "GL")
nb.GL <- gam(Density ~ s(bathy, k=3) + s(dist, k = 3) + s(temp, k=4), family = nb, data=GL_train)
GL_test$predicted <- predict(nb.GL, newdata = GL_test, type = "response")

CM_train <- train %>% filter(species == "CoMu")
CM_test <- test %>% filter(species == "CoMu")
nb.CM <- gam(Density ~ s(dist, k=3) + s(temp, k=4) + s(salt, k=4) + s(bathy, k=3), family = nb, data=CM_train)
CM_test$predicted <- predict(nb.CM, newdata = CM_test, type = "response")

seabird_21 <- rbind(GL_test, CM_test)
```
Adding predictions to marine mammals:
```{r Mammal Predictions}
HPorp_train <- train %>% filter(species == "HPorp")
HPorp_test <- test %>% filter(species == "HPorp")
HP <- glm(PA ~ dist, data= HPorp_train, family = "binomial") 
HPorp_test$predicted <- predict(HP, newdata = HPorp_test, type = "response")

HSeal_train <- train %>% filter(species == "HSeal")
HSeal_test <- test %>% filter(species == "HSeal")
HS <- glm(PA ~ bathy + channel_width + phyto + tcur, data= HSeal_train, family = "binomial")
HSeal_test$predicted <- predict(HS, newdata = HSeal_test, type = "response")
```

# Step 3: Assess the accuacy of the model predictions
This next chunk of code will calculate the correlation coefficient and r2 between observed density and predicted density for seabird models
```{r Seabirds: Density vs. Probability}
rsq <- function (x, y) cor(x, y) ^ 2

cor(GL_test$Density, GL_test$predicted)
rsq(GL_test$Density, GL_test$predicted)

cor(CM_test$Density, CM_test$predicted)
rsq(CM_test$Density, CM_test$predicted)

# NOTE: The correlation between the predicted and observed data is quite low when considered at this scale. This is potentially because both of these models are based primarily on static environmental conditions and are therefore ill-suited for predicting fine scale variability. The original research question was: Based on the spatial differences in abundance observed along our transect line, can we predict where marine birds and mammals will be GENERALLY more abundant across the wider archipeligo. Thus, we need to compare how good the models are at differentiating abundance between the different transect zones. 

# I will be implementing a paired t-test to compare observations vs. predictions. For more information, read: 
# http://www.sthda.com/english/wiki/paired-samples-t-test-in-r 

t.test(GL_test$predicted, GL_test$Density, paired = TRUE, alternative = "two.sided")
t.test(CM_test$predicted, CM_test$Density, paired = TRUE, alternative = "two.sided")

```
The next chunk of code will calculate the accuracy of my presence / absense logistic regression models - looking at the AUC of the response operating curve. For more information, follow this link: https://www.statology.org/auc-in-r/

```{r MarMam: AUC}
# calculating AUC
auc(HSeal_test$PA, HSeal_test$predicted)
ci.auc(HSeal_test$PA, HSeal_test$predicted)

auc(HPorp_test$PA, HPorp_test$predicted)
ci.auc(HPorp_test$PA, HPorp_test$predicted)
```

# Step 4: Put it all together
This code chunk will run 7-fold validation on the cruise data 
```{r 7-fold validation}
allrows <- 1:nrow(cruises_ocean_time_all)

set.seed(7)
sample.rows <- sample(allrows, replace = F, size = length(allrows))

seabird.val <- data.frame()
marmam.val <- data.frame()

for (i in 1:7) {
  # step 1: compile the training and testing data
  train_rows <- sample.rows[(((i-1)*4)+1):(i*4)]
  test_rows <- allrows[-train_rows]
  train <- cruises_ocean_time_all[train_rows,]
  train <- train %>% dplyr::select(-c(ocean_time))
  train$Date <- ymd(train$Date)
  test <- cruises_ocean_time_all[test_rows,]
  test <- test %>% dplyr::select(-c(ocean_time))
  test$Date <- ymd(test$Date)
  train <- inner_join(trans_1, train)
  test <- inner_join(trans_1, test)
  print("Finished Step 1")
  # step 2: add predicted values
  GL_train <- train %>% filter(species == "GL")
  GL_test <- test %>% filter(species == "GL")
  ps.GL <- gam(count ~ s(bathy, k=3) + s(channel_width, k=3) + s(temp, k=4) + s(temp_sd, k=5) + s(salt, k=4) + s(topog, k=3), offset = log(effort), family = poisson(link = "log"), data=GL_train)
  #nb.GL <- gam(Density ~ s(bathy, k=3) + s(dist, k = 3) + s(temp, k=4), family = nb, data=GL_train)
  GL_test$predicted <- predict(ps.GL, newdata = GL_test, type = "response")
  #GL_test$predicted <- predict(nb.GL, newdata = GL_test, type = "response")
  
  CM_train <- train %>% filter(species == "CoMu")
  CM_test <- test %>% filter(species == "CoMu")
  ps.CM <- gam(count ~ s(dist, k=3) + s(phyto, k=4) + s(bathy, k=3) + s(temp_sd, k=5) + s(tcur, k=3), offset = log(effort), family = poisson(link = "log"), data=CoMu)
  #nb.CM <- gam(Density ~ s(dist, k=3) + s(temp, k=4) + s(salt, k=4) + s(bathy, k=3), family = nb, data=CM_train)
  CM_test$predicted <- predict(ps.CM, newdata = CM_test, type = "response")
  #CM_test$predicted <- predict(nb.CM, newdata = CM_test, type = "response")
  
  HPorp_train <- train %>% filter(species == "HPorp")
  HPorp_test <- test %>% filter(species == "HPorp")
  HP <- glm(PA ~ dist, data= HPorp_train, family = "binomial") 
  HPorp_test$predicted <- predict(HP, newdata = HPorp_test, type = "response")
  
  HSeal_train <- train %>% filter(species == "HSeal")
  HSeal_test <- test %>% filter(species == "HSeal")
  HS <- glm(PA ~ bathy + channel_width + phyto + tcur, data= HSeal_train, family = "binomial")
  HSeal_test$predicted <- predict(HS, newdata = HSeal_test, type = "response")
  print("Finished Step 2")
  # step 3: assess the accuacy of the models
  # abundance models
  GL.t <- t.test(GL_test$predicted, GL_test$Density, paired = TRUE, alternative = "two.sided")
  CM.t <- t.test(CM_test$predicted, CM_test$Density, paired = TRUE, alternative = "two.sided")

  data <- data.frame(
    Trial = c(i,i),
    cor.coef = c(cor(GL_test$Density, GL_test$predicted), cor(CM_test$Density, CM_test$predicted)),
    r.squared = c(rsq(GL_test$Density, GL_test$predicted), rsq(CM_test$Density, CM_test$predicted)),
    p.value = c(GL.t$p.value, CM.t$p.value),
    species = c("GL", "CoMu")
  )
  seabird.val <- rbind(seabird.val, data)
  rm(GL.t, CM.t, data)
  print("Finished Step 3a")
  # presence absence models
  HS.t <- ci.auc(HSeal_test$PA, HSeal_test$predicted)
  HP.t <- ci.auc(HPorp_test$PA, HPorp_test$predicted)
  
  data <- data.frame(
    AUC = c(HS.t[2], HP.t[2]),
    L.CI = c(HS.t[1], HP.t[1]),
    H.CI = c(HS.t[3], HP.t[3]),
    species = c("HSeal", "HPorp")
  )
  
 marmam.val <- rbind(marmam.val, data)
 rm(HS.t, HP.t, data)
 print("Finished Step 3b")
 
 print(i)
}
```

```{r stratified Monte Carlo Validation}
rows_2017 <- 1:6
rows_2018 <- 7:12
rows_2019 <- 13:17
rows_2020 <- 18:24
rows_2021 <- 25:28

seabird.eval <- data.frame()
seabird.coef <- data.frame()

marmam.eval <- data.frame()
marmam.coef <- data.frame()

t1 <- Sys.time()

for (i in 1:500) {
  # step 1: compile the training and testing data
  set.seed(i)
  strat.2017 <- sample(rows_2017, replace = F, size = 0.75*length(rows_2017))
  strat.2018 <- sample(rows_2018, replace = F, size = 0.75*length(rows_2018))
  strat.2019 <- sample(rows_2019, replace = F, size = 0.75*length(rows_2019))
  strat.2020 <- sample(rows_2020, replace = F, size = 0.75*length(rows_2020))
  strat.2021 <- sample(rows_2021, replace = F, size = 0.75*length(rows_2021))
  
  train_rows <- c(strat.2017, strat.2018, strat.2019, strat.2020, strat.2021)
  rm(strat.2017, strat.2018, strat.2019, strat.2020, strat.2021)
  test_rows <- allrows[-train_rows]
  train <- cruises_ocean_time_all[train_rows,]
  train <- train %>% dplyr::select(-c(ocean_time))
  train$Date <- lubridate::ymd(train$Date)
  test <- cruises_ocean_time_all[test_rows,]
  test <- test %>% dplyr::select(-c(ocean_time))
  test$Date <- lubridate::ymd(test$Date)
  train <- inner_join(trans_1, train)
  test <- inner_join(trans_1, test)
  if(!1 %in% unique(train$cruise.gen)){test <- filter(test, cruise.gen !=1)}
  if(!2 %in% unique(train$cruise.gen)){test <- filter(test, cruise.gen !=2)}
  if(!3 %in% unique(train$cruise.gen)){test <- filter(test, cruise.gen !=3)}
  if(!4 %in% unique(train$cruise.gen)){test <- filter(test, cruise.gen !=4)}
  if(!5 %in% unique(train$cruise.gen)){test <- filter(test, cruise.gen !=5)}
  if(!6 %in% unique(train$cruise.gen)){test <- filter(test, cruise.gen !=6)}
  if(!7 %in% unique(train$cruise.gen)){test <- filter(test, cruise.gen !=7)}
  print("Finished Step 1")
  # step 2: add predicted values NOTE: SELECT THE DESIRED MODEL ON LINES 3327:3330 AND 3335:3338
  GL_train <- train %>% filter(species == "GL")
  GL_test <- test %>% filter(species == "GL")
  nb.GL <- gam(Density ~ s(temp, k = 4) + s(Zone, bs = "re") + s(Year, bs = "re"), family = nb, data=GL_train)
  GL_test$predicted <- predict(nb.GL, newdata = GL_test, type = "response")
  GL_test$count[GL_test$count == 0] <- 0.001
  
  CM_train <- train %>% filter(species == "CoMu")
  CM_test <- test %>% filter(species == "CoMu")
  nb.CM <- gam(Density ~ s(dth, k = 4) + s(phyto, k = 3) + s(Zone, bs = "re") + s(cruise.gen, bs = "re") + s(Year, bs = "re"), family = nb, data=CM_train)
  CM_test$predicted <- predict(nb.CM, newdata = CM_test, type = "response")
  CM_test$count[CM_test$count == 0] <- 0.001
  
  HPorp_train <- train %>% filter(species == "HPorp")
  HPorp_test <- test %>% filter(species == "HPorp")
  glm.HP <- glm(PA ~ temp_sd, data= HPorp_train, family = "binomial")
  glm0.0 <- update(glm.HP, . ~ 1)
  HPorp_test$predicted <- predict(glm.HP, newdata = HPorp_test, type = "response")
  
  HSeal_train <- train %>% filter(species == "HSeal")
  HSeal_test <- test %>% filter(species == "HSeal")
  glm.HS <- glmer(PA ~ dth + (1 | Year) + (1 | Zone) + (1 | cruise.gen), data= HSeal_train, family = "binomial")
  glm0 <- update(glm.HS, . ~ 1 + (1 | Year) + (1 | Zone) + (1 | cruise.gen))
  HSeal_test$predicted <- predict(glm.HS, newdata = HSeal_test, type = "response")
  print("Finished Step 2")
  # step 3: assess the accuracy of the models
  # abundance models
  GL.mi <- min(((GL_test$count - GL_test$predicted)/GL_test$count) * 100, na.rm = TRUE)
  GL.me <- median(((GL_test$count - GL_test$predicted)/GL_test$count) * 100, na.rm = TRUE)
  GL.ma <- max(((GL_test$count - GL_test$predicted)/GL_test$count) * 100, na.rm = TRUE)
  GL.rsq <- summary(nb.GL)$r.sq
  GL.dex <- summary(nb.GL)$dev.expl
  
  CM.mi <- min(((CM_test$count - CM_test$predicted)/CM_test$count) * 100, na.rm = TRUE)
  CM.me <- median(((CM_test$count - CM_test$predicted)/CM_test$count) * 100, na.rm = TRUE)
  CM.ma <- max(((CM_test$count - CM_test$predicted)/CM_test$count) * 100, na.rm = TRUE)
  CM.rsq <- summary(nb.CM)$r.sq
  CM.dex <- summary(nb.CM)$dev.expl

  data.eval <- data.frame(
    Trial = c(i,i), 
    species = c("GL", "CoMu"), 
    min.pe = c(GL.mi, CM.mi), 
    med.pe = c(GL.me, CM.me), 
    max.pe = c(GL.ma, CM.ma), 
    r.squ = c(GL.rsq, CM.rsq), 
    dev.ex = c(GL.dex, CM.dex),
    test = c(nrow(GL_test), nrow(CM_test))
  )
  
  coef <- nb.GL$coefficients
  data.cov <- stack(coef)
  data.cov$iter <- i
  data.cov$species <- "GL"
  coef <- nb.CM$coefficients
  coef <- stack(coef)
  coef$iter <- i
  coef$species <- "CM"
  data.cov <- rbind(data.cov, coef)
  
  seabird.eval <- rbind(seabird.eval, data.eval)
  seabird.coef <- rbind(seabird.coef, data.cov)
  rm(GL.mi, CM.mi, GL.me, CM.me, GL.ma, CM.ma, GL.rsq, CM.rsq, GL.dex, CM.dex, coef, data.cov)
  print("Finished Step 3a")
  
  # presence absence models
  HS.t <- ci.auc(HSeal_test$PA, HSeal_test$predicted)
  HP.t <- ci.auc(HPorp_test$PA, HPorp_test$predicted)
  
  HS.rsq <- 1-logLik(glm.HS)/logLik(glm0)
  HP.rsq <- 1-logLik(glm.HP)/logLik(glm0.0)

  HS.dex <- 1 - (deviance(glm.HS)/deviance(glm0))
  HP.dex <- 1 - (summary(glm.HP)$deviance/ summary(glm.HP)$null.deviance)
    
  data.eval <- data.frame(
    Trial = c(i,i),
    AUC = c(HS.t[2], HP.t[2]),
    L.CI = c(HS.t[1], HP.t[1]),
    H.CI = c(HS.t[3], HP.t[3]),
    r.sq = c(HS.rsq, HP.rsq),
    dev.ex = c(HS.dex, HP.dex),
    species = c("HSeal", "HPorp")
    
  )
  
  coef <- fixef(glm.HS)
  data.cov <- stack(coef)
  data.cov$iter <- i
  data.cov$species <- "HS"
  coef <- glm.HP$coefficients
  coef <- stack(coef)
  coef$iter <- i
  coef$species <- "HP"
  data.cov <- rbind(data.cov, coef)
  
 marmam.eval <- rbind(marmam.eval, data.eval)
 marmam.coef <- rbind(marmam.coef, data.cov)
 rm(HS.t, HP.t, HS.rsq, HP.rsq, HS.dex, HP.dex, data.eval, data.cov, coef)
 print("Finished Step 3b")
 t2 <- Sys.time()
 print(i)
}
```

# LiveOcean Data Validation
```{r LiveOcean Data Validation}
# input all the measured salinity and temperature data from CTD casts 
LiveOcean_validation <- data.frame(
  Date = c("2017-10-03", "2017-10-03", "2017-10-10", "2017-10-10", "2017-10-19", "2017-10-24", "2017-10-24", "2017-10-31", "2017-10-31", "2017-11-07", "2017-11-07", "2017-11-16", "2017-11-16", "2018-10-02", "2018-10-02", "2018-10-11", "2018-10-11", "2018-10-11", "2018-10-16", "2018-10-16", "2018-10-16", "2018-10-23", "2018-10-23", "2018-10-23", "2018-10-30", "2018-10-30", "2018-10-30", "2018-11-06", "2018-11-06", "2018-11-06","2019-10-12", "2019-10-12", "2019-10-19", "2019-10-19", "2019-10-23", "2019-10-23", "2019-10-30", "2019-10-30", "2019-11-05", "2019-11-05","2020-10-08", "2020-10-08", "2020-10-08", "2020-10-15", "2020-10-15", "2020-10-15", "2020-10-20", "2020-10-20", "2020-10-20", "2020-10-26", "2020-10-26", "2020-10-26", "2020-11-02", "2020-11-02", "2020-11-02", "2020-11-10", "2020-11-10", "2020-11-10", "2021-10-07", "2021-10-07", "2021-10-12", "2021-10-12", "2021-10-19", "2021-10-19", "2021-10-26", "2021-10-26", "2021-10-26", "2021-11-02", "2021-11-02", "2021-11-11", "2021-11-11"),
  Station = c("N", "S", "N", "S", "N", "N", "S", "N", "S", "N", "S", "N", "S", "N", "S", "N", "N", "S", "N", "N", "S", "N", "N", "S", "N", "N", "S", "N", "N", "S", "N", "S", "N", "S", "N", "S", "N", "N", "N", "N", "N", "N", "N", "N", "N", "S", "N", "N", "S", "N", "N", "S", "N", "N", "S", "N", "N", "S", "N", "N", "N", "S", "N", "S", "N", "N", "S", "N", "S", "N", "S"),
  ObsTemp = c(11.1744,11.0054,10.7027,10.1144,10.0385,9.8823,9.995,9.9699,9.9854,9.3791,9.0527,9.11,9.0302,10.9532,10.9468,10.5011,10.6542,10.1065,11.087,11.0187,10.5294,10.39,10.3256,10.4205,9.9189,10.0585,10.025,9.9449,10.0165,9.9937,10.6902,10.4618,10.0837,9.9213,10.0263,10.2864,10.0044,9.8365,9.938,9.8422,11.5242,11.1247,11.5916,10.2834,10.2534,10.3672,10.2935,10.2982,10.0267,9.7976,9.8429,9.7983,9.8346,9.8355,9.7053,9.2335,9.5171,9.1307,10.309,10.6499,10.4456,10.1292,10.1141,10.0473,9.8923,9.8959,9.8819,9.8955,9.8722,9.667,9.6359),
  ObsSalt = c(29.0343,30.2634,29.7609,31.1907,30.799,31.1091,31.2633,28.9391,30.2145,30.5538,31.2972,30.9555,31.1721,30.1833,30.677,30.0539,30.246,31.2808,28.8436,29.4449,30.9646,30.2389,30.4773,30.6032,31.1851,30.9125,31.2325,30.867,30.9095,31.158,29.6213,30.6697,30.9051,31.1349,31.4226,30.8845,30.031,30.1729,29.3683,30.2707,28.3206,29.1353,28.3167,30.4835,30.7631,30.7125,29.5337,30.1982,31.0804,29.9763,30.3453,30.365,29.8275,30.0771,30.731,28.4766,28.2706,29.89,30.0563,30.2651,29.2604,31.1413,30.8022,30.7752,30.9859,30.7487,31.2495,29.0327,29.8177,30.4858,30.4644)
)

model_pt <- data.frame(
  Station = c("N", "N", "N", "N", "S", "S", "S", "S"),
  lat.x = c(48.5825895797052,48.5825895797052,48.5870894726895,48.5870894726895, 48.4205934322702, 48.4205934322702,48.4160935392859,48.4250933252545),
  lon.x = c(-123.038688775285,-123.045421593992,-123.038688775285,-123.045421593992,-122.944429313385,-122.937696494678,-122.944429313385,-122.944429313385)
)

# combine LiveOcean data from all years
temp_test <- rbind(temp_2017, temp_2018, temp_2019, temp_2020, temp_2021)
salt_test <- rbind(sal_2017, sal_2018, sal_2019, sal_2020, sal_2021)

# convert coordinates to characters for joining
temp_test$lat.x <- as.character(temp_test$lat.x)
temp_test$lon.x <- as.character(temp_test$lon.x)

salt_test$lat.x <- as.character(salt_test$lat.x)
salt_test$lon.x <- as.character(salt_test$lon.x)

model_pt$lat.x <- as.character(model_pt$lat.x)
model_pt$lon.x <- as.character(model_pt$lon.x)

# identify 4 closest model points to each station
temp_test <- inner_join(temp_test, model_pt)
salt_test <- inner_join(salt_test, model_pt)

# convert ocean_time to dates 
temp_test <- temp_test %>% mutate(Date = as.Date(ocean_time, origin = "2016-12-31"))
salt_test <- salt_test %>% mutate(Date = as.Date(ocean_time, origin= "2016-12-31"))

# average the modeled values for the 4 closest points to each station
temp_test <- temp_test %>% group_by(Date, Station) %>% summarise(ModTemp = mean(temp))
salt_test <- salt_test %>% group_by(Date, Station) %>% summarise(ModSalt = mean(salt))

# convert Date values to character for join
temp_test$Date <- as.character(temp_test$Date)
salt_test$Date <- as.character(salt_test$Date)

# join LiveOcean model data with measured CTD data
LiveOcean_validation <- inner_join(LiveOcean_validation, temp_test)
LiveOcean_validation <- inner_join(LiveOcean_validation, salt_test)

# run the paired t-test
# TEMPERATURE #
t.test(LiveOcean_validation$ObsTemp, LiveOcean_validation$ModTemp, paired = TRUE, alternative = "two.sided")
# SALINITY #
t.test(LiveOcean_validation$ObsSalt, LiveOcean_validation$ModSalt, paired = TRUE, alternative = "two.sided")

```

## CORE HABITAT PREDICTIONS
```{r Predicted Habitat Map}
## Predicting Across Study Grid Using Environmental Variables ##
# compile prediction space
scaled.env <- env_all

far <- scaled.env %>% filter(dist > 2497.3)
far$edit <- 1
far$dist <- 2497.3
near <- scaled.env %>% filter(dist <= 2497.3)
near$edit <- 0
scaled.env <- rbind(far, near)
rm(far, near)

front <- scaled.env %>% filter(temp_sd > 0.3218)
front$temp_sd <- 0.3218
front$edit <- 1
back <- scaled.env %>% filter(temp_sd <= 0.3218)
scaled.env <- rbind(front, back)
rm(front, back)

scaled.vars <- scale(scaled.env[,c(6:10,12:15)])
scaled.env <- scaled.env %>% dplyr::select(c("grid_id", "lat_deg", "long_deg", "Date", "edit"))
scaled.env <- cbind(scaled.env, scaled.vars)
rm(scaled.vars)

scaled.env$dup <- scaled.env$Date
scaled.env <- scaled.env %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
scaled.env$Year <- factor(scaled.env$Year)

scaled.env <- scaled.env %>% group_by(grid_id, Year) %>% summarize(lat_deg = mean(lat_deg, na.rm = TRUE), long_deg = mean(long_deg, na.rm = TRUE), bathy = mean(bathy, na.rm = TRUE), topog = mean(topog, na.rm = TRUE), dist = mean(dist, na.rm = TRUE), chwi = mean(channel_width, na.rm = TRUE), tcur = mean(tcur, na.rm = TRUE), phyto = mean(phyto, na.rm = TRUE), temp = mean(temp, na.rm = TRUE), temp_sd = mean(temp_sd, na.rm = TRUE), salt = mean(salt, na.rm = TRUE), edit = max(edit))

# make the predictions
scaled.env$GL <- predict(lm.GL111.01, newdata = scaled.env, type = "response")
scaled.env$CM <- predict(lm.CM1111.02, newdata = scaled.env, type = "response")
scaled.env$HS <- predict(lm.HS3.4, newdata = scaled.env, type = "response")

scaled.env <- scaled.env %>% group_by(grid_id) %>% summarise(long_deg = mean(long_deg), lat_deg = mean(lat_deg), GL = mean(GL), GL.s = sd(GL, na.rm = TRUE), CM = mean(CM, na.rm = TRUE), CM.s = sd(CM, na.rm = TRUE), HS = mean(HS), HS.sd = sd(HS, na.rm = TRUE), edit = max(edit))

CV <- ungroup(scaled.env) %>% group_by(grid_id) %>% summarise(GL.m = mean(GL, na.rm = TRUE), CM.m = mean(CM, na.rm = TRUE), HS.m = mean(HS, na.rm = TRUE), GL.sd = sd(GL, na.rm = TRUE), CM.sd = sd(CM, na.rm = TRUE), HS.sd = sd(HS, na.rm = TRUE))

scaled.env.sf <- scaled.env %>% st_as_sf(coords = c("long_deg", "lat_deg"), crs = 4326)

# visualize
## GLAUCOUS GULL ##
ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = scaled.env.sf, aes(fill = GL), size = 2.75, shape = 22) +
  scale_fill_viridis(option = "magma", name = "Count (#)") +
  ggtitle("Glaucous W. Gull Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal() 
  theme(legend.position="bottom")
## COMMON MURRE ##
ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = scaled.env.sf, aes(fill = CM), size = 2.75, shape = 22) +
  scale_fill_viridis(option = "magma", name = "Count(#)") +
  ggtitle("Common Murre Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal()
## HARBOR SEAL ##
ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = scaled.env.sf, aes(fill = HS), size = 2.75, shape = 22) +
  scale_fill_viridis(option = "magma", name = "Count (#)") +
  ggtitle("Harbor Seal Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal()

# PERCENTILE #

# GL
GL.per <- c(
  scaled.env$GL %>% quantile(1), # 33.62538 
  scaled.env$GL %>% quantile(0.9), # 17.01541
  scaled.env$GL %>% quantile(0.7), # 12.90835
  scaled.env$GL %>% quantile(0.3), # 7.029895  
  scaled.env$GL %>% quantile(0.1),# 3.407768
  0
)
# CM
CM.per <- c(
  scaled.env$CM %>% quantile(1), # 665.4393 
  scaled.env$CM %>% quantile(0.9), # 426.659
  scaled.env$CM %>% quantile(0.7), # 210.1348 
  scaled.env$CM %>% quantile(0.3), # 18.11892  
  scaled.env$CM %>% quantile(0.1), # 5.006768
  0
)

# HS
HS.per <- c(
  scaled.env$HS %>% quantile(1), # 2.254061 
  scaled.env$HS %>% quantile(0.9), # 2.121943 
  scaled.env$HS %>% quantile(0.7), # 1.82 
  scaled.env$HS %>% quantile(0.3), # 0.9198168  
  scaled.env$HS %>% quantile(0.1), # 0.02784073
  0
)

scaled.env <- ungroup(scaled.env) %>% mutate(
  GL.p = cut(GL, breaks = GL.per, labels = c(-2,-1,0,1,2)), 
  CM.p = cut(CM, breaks = CM.per, labels = c(-2,-1,0,1,2)),
  HS.p = cut(HS, breaks = HS.per, labels = c(-2,-1,0,1,2))
)

scaled.env.sf <- scaled.env %>% st_as_sf(coords = c("long_deg", "lat_deg"), crs = 4326)

# visualize
## GLAUCOUS GULL ##
ggplot() +
  geom_sf(data = islands, fill = "grey") +
  geom_sf(data = scaled.env.sf, aes(fill = GL.p), size = 2.75, shape = 22) +
  scale_fill_cmocean(name = "balance", discrete = TRUE, labels = c("10", "30", "50", "70", "90")) +
  ggtitle("Glaucous W. Gull Predicted Habitat (Autumn)") +
  xlab("Longitude (ºW)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal() +
  theme(legend.position="bottom")
## COMMON MURRE ##
ggplot() +
  geom_sf(data = islands, fill = "grey") +
  geom_sf(data = scaled.env.sf, aes(fill = CM.p), size = 2.75, shape = 22) +
  scale_fill_cmocean(name = "balance", discrete = TRUE) +
  ggtitle("Common Murre Predicted Habitat (Autumn)") +
  xlab("Longitude (ºW)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal() + 
  theme(legend.position="bottom")
## HARBOR SEAL ##
ggplot() +
  geom_sf(data = islands, fill = "grey") +
  geom_sf(data = scaled.env.sf, aes(fill = HS.p), size = 2.75, shape = 22) +
  scale_fill_cmocean(name = "balance", discrete = TRUE) + 
  ggtitle("Harbor Seal Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal() +
  theme(legend.position="bottom")

```


## PAM CLUSTER ANALYSIS
Lets try using the k-medoid method in place of k-means method to see how much outliers influence the deliniation of environmental clusters

```{r PAM cluster}
# get variables for clustering
env_cluster <-  env_all %>% dplyr::select(grid_id, lat_deg, long_deg, bathy, topog, dist, channel_width, tcur, phyto, temp, temp_sd, salt)

env_cluster <- env_cluster %>% group_by(grid_id) %>% summarise(
  lat_deg = mean(lat_deg),
  long_deg = mean(long_deg),
  bathy = mean(bathy),
  topog = mean(topog),
  dist = mean(dist),
  channel_width = mean(channel_width),
  tcur = mean(tcur),
  phyto = mean(phyto),
  temp = mean(temp),
  temp_sd = mean(temp_sd),
  salt = mean(salt)
)

env_cluster <- env_cluster %>% filter(!is.na(temp_sd))
clust_vars <- env_cluster %>% dplyr::select(-c(grid_id))

# standardize the variables to a mean of 0 and standard deviation of 1
clust_vars_standardized <- as_tibble(scale(clust_vars))  

# run the clustering
pam.res <- pam(clust_vars_standardized, k=15, metric = "euclidean", stand = FALSE)
env_cluster$cluster <- pam.res$cluster
env_cluster$cluster <- factor(env_cluster$cluster)

# now let's look at the results

cluster_sf <- env_cluster %>% st_as_sf(coords = c("long_deg", "lat_deg"), crs = 4326)

ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = cluster_sf, aes(fill = cluster), size = 3, shape = 22) +
  geom_sf(data= transect) +
  coord_sf() +
  xlab("Longitude") +
  ylab("Latitude") +
  theme_minimal()

fviz_nbclust(clust_vars_standardized, method = "silhouette", FUNcluster =  pam)
fviz_nbclust(clust_vars_standardized, method = "wss", FUNcluster =  pam)
fviz_nbclust(clust_vars_standardized, method = "gap_stat", FUNcluster =  pam)

# According to Milligan and Cooper (1985), CH index, Duda index, Cindex, Gamma, and Beale tests are the indicies which performed best in simulations

NbClust(data = clust_vars_standardized, distance = "euclidean", method = "kmeans", index = "ch")
NbClust(data = clust_vars_standardized, distance = "euclidean", method = "kmeans", index = "duda")
NbClust(data = clust_vars_standardized, distance = "euclidean", method = "kmeans", index = "cindex")
NbClust(data = clust_vars_standardized, distance = "euclidean", method = "kmeans", index = "beale")

```

## CALCULATING MODEL EXTRAPOLATION
The package "dsmextra" can be used to calculate model extrapolation using the methods described by Mesgaran et al.,(2014).
Read more about the package here:
https://densitymodelling.github.io/dsmextra/articles/dsmextra.html

```{r dsmextra data}
# compile function imputs
aftt_crs <- sp::CRS("+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs+ towgs84=0,0,0") # NAD83
covariates.sja <- c("bathy", "dist", "temp", "phyto", "temp_sd", "salt", "topog")

# add spatial data to training inputs
transect_bounds <- read_csv("~/Desktop/PEF/SJA_StudyArea/transect_bounds.csv")
transect_midpoints <- data.frame(
  Zone = factor(c(1,2,3,4,5,6)),
  y = c(48.56946, 48.55147, 48.52447, 48.48849, 48.45250, 48.42551),
  x = c(-123.0209, -122.9802, -122.9532, -122.9532, -122.9532, -122.9397)
)

sample <- trans_all %>% filter(!is.na(phyto))
sample$dup <- sample$Date
sample <- sample %>% separate(dup, into = c("Year", "Month", "Day"), sep = "([-])")
sample$Year <- factor(sample$Year)
sample <- sample %>% group_by(Year, Zone) %>% summarise(bathy = mean(bathy), topog = mean(topog), dist = mean(dist), chwi = mean(channel_width), tcur = mean(tcur), phyto = mean(phyto), temp = mean(temp), temp_sd = mean(temp_sd), salt = mean(salt))

# create prediction grid object
predgrid <- env_all 
predgrid <- predgrid %>% group_by(grid_id) %>% summarise(
  x = mean(long_deg),
  y = mean(lat_deg),
  bathy = mean(bathy),
  topog = mean(topog),
  dist = mean(dist),
  channel_width = mean(channel_width),
  tcur = mean(tcur),
  phyto = mean(phyto),
  temp = mean(temp),
  temp_sd = mean(temp_sd, na.rm = TRUE),
  salt = mean(salt)
)

# manually compare predgrid and sample
predgrid %>%
  dplyr::filter(!dplyr::between(bathy, min(sample$bathy), max(sample$bathy)) |
                  !dplyr::between(dist, min(sample$dist), max(sample$dist)) |
                  !dplyr::between(temp_sd, min(sample$temp_sd), max(sample$temp_sd)) |
                  !dplyr::between(temp, min(sample$temp), max(sample$temp)) |
                  !dplyr::between(phyto, min(sample$phyto), max(sample$phyto)) |
                  !dplyr::between(topog, min(sample$topog), max(sample$topog)) |
                  !dplyr::between(salt, min(sample$salt), max(sample$salt))) %>%
  nrow()
```

now run the analysis
```{r dsmextra analysis}
# calculate extrapolation
sja.extrapolation <- compute_extrapolation(samples = sample,
                                   covariate.names = covariates.sja,
                                   prediction.grid = predgrid,
                                   coordinate.system = aftt_crs,
                                   resolution = c(1/74, 1/111))

summary(sja.extrapolation)

# visualize results
r_df <- as.data.frame(sja.extrapolation$rasters$ExDet$all, xy = TRUE)

ggplot(data = r_df) +
  geom_raster(aes(x = x, y = y, fill = ExDet)) +
  scale_fill_gradient2() +
  coord_quickmap() +
  theme_classic()
```

```{r mask for model extrapolation}
ex_mask <- r_df
# remove areas with small Mahalanobis metic values
ex_mask[ex_mask >-1 & ex_mask < 1] <- 0
ex_mask <- ex_mask %>% filter(ExDet != 0)
# apply the mask to prediction spaces

# GL
ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = GL_Predicted_Presence, aes(fill = predicted), size = 2.75, shape = 22) +
  geom_tile(data = ex_mask, aes(x = x, y = y), fill = "white", alpha = 0.4) +
  scale_fill_viridis(option = "magma", name = "Density") +
  ggtitle("Glaucous W. Gull Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal() 

# CoMu
ggplot() +
  geom_sf(data = islands) +
  geom_sf(data = CM_Predicted_Presence, aes(fill = predicted), size = 2.75, shape = 22) +
  geom_tile(data = ex_mask, aes(x = x, y = y), fill = "white", alpha = 0.4) +
  scale_fill_viridis(option = "magma", name = "Density") +
  ggtitle("Common Murre Predicted Habitat (Autumn)") +
  xlab("Longitude (ºE)") +
  ylab("Latitude (ºN)") +
  coord_sf() +
  theme_minimal()
```

